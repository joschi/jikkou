[{"body":" Hands-on: Try the Jikkou: Get Started tutorials.\nThe command line interface to Jikkou is the jikkou command, which accepts a variety of subcommands such as jikkou apply or jikkou validate.\nTo view a list of the commands available in your current Jikkou version, run jikkou with no additional arguments:\nUsage: jikkou [-hV] [COMMAND] Jikkou CLI:: A command-line client designed to provide an efficient and easy way to manage, automate, and provision resources for any kafka infrastructure. Find more information at: https://streamthoughts.github.io/jikkou/. Options: -h, --help Show this help message and exit. -V, --version Print version information and exit. Commands: create Create resources from the resource definition files (only non-existing resources will be created). delete Delete resources that are no longer described by the resource definition files. update Create or update resources from the resource definition files apply Update the resources as described by the resource definition files. resources List supported resources extensions List or describe the extensions of Jikkou config Sets or retrieves the configuration of this client diff Display all resource changes. validate Validate resource definition files. health Print or describe health indicators. help Display help information about the specified command. get List and describe all resources of a specific kind. (The output from your current Jikkou version may be different than the above example.)\nChecking Jikkou Version Run the jikkou --version to display your current installation version:\nJikkou version \"0.29.0\" 2023-09-29 JVM: 17.0.7 (Oracle Corporation Substrate VM 17.0.7+8-LTS) Shell Tab-completion It is recommended to install the bash/zsh completion script jikkou_completion.\nThe completion script can be downloaded from the project Github repository:\nwget https://raw.githubusercontent.com/streamthoughts/jikkou/main/jikkou_completion . jikkou_completion or alternatively, you can run the following command to generate it.\nsource \u003c(jikkou generate-completion) ","categories":"","description":"","excerpt":" Hands-on: Try the Jikkou: Get Started tutorials.\nThe command line ‚Ä¶","ref":"/jikkou/docs/jikkou-cli/basic-cli-features/","tags":"","title":"Basic CLI Features"},{"body":"You can use a ConfigMap to define reusable data in the form of key/value pairs that can then be referenced and used by other resources.\nSpecification --- apiVersion: \"core.jikkou.io/v1beta2\" kind: ConfigMap metadata: name: '\u003cCONFIG-MAP-NAME\u003e' # Name of the ConfigMap (required) data: # Map of key-value pairs (required) \u003cKEY_1\u003e: \"\u003cVALUE_1\u003e\" Example For example, the below ConfigMap show how to define default config properties namedcKafkaTopicConfig that can then reference and used to define multiple KafkaTopic. resources.\n--- apiVersion: \"core.jikkou.io/v1beta2\" kind: ConfigMap metadata: name: 'KafkaTopicConfig' data: cleanup.policy: 'delete' min.insync.replicas: 2 retention.ms: 86400000 # (1 day) ","categories":"","description":"Learn how to use ConfigMap objects.\n","excerpt":"Learn how to use ConfigMap objects.\n","ref":"/jikkou/docs/user-guide/core/resources/configmap/","tags":"","title":"ConfigMap"},{"body":"This document will guide you through setting up Jikkou in a few minutes and managing your first resources with Jikkou.\nPrerequisites The following prerequisites are required for a successful and properly use of Jikkou.\nMake sure the following is installed:\nAn Apache Kafka cluster. Using Docker, Docker Compose is the easiest way to use it. Java 17 (not required when using the binary version). Start your local Apache Kafka Cluster You must have access to an Apache Kafka cluster for using Jikkou. Most of the time, the latest version of Jikkou is always built for working with the most recent version of Apache Kafka.\nMake sure the Docker is up and running.\nThen, run the following commands:\n$ git clone https://github.com/streamthoughts/jikkou $ cd jikkou $ ./up # use ./down for stopping the docker-compose stack Run Jikkou Download the latest distribution (For Linux) Run the following commands to install the latest version:\nwget https://github.com/streamthoughts/jikkou/releases/download/v0.26.0/jikkou-0.26.0-linux-x86_64.zip \u0026\u0026 \\ unzip jikkou-0.26.0-linux-x86_64.zip \u0026\u0026 \\ cp jikkou-0.26.0-linux-x86_64/bin/jikkou $HOME/.local/bin \u0026\u0026 \\ source \u003c(jikkou generate-completion) \u0026\u0026 \\ jikkou --version For more details, or for other options, see the installation guide.\nConfigure Jikkou for your local Apache Kafka cluster Set configuration context for localhost\njikkou config set-context localhost --config-props=kafka.client.bootstrap.servers=localhost:9092 Show the complete configuration.\njikkou config view --name localhost Finally, let‚Äôs check if your cluster is accessible:\njikkou health get kafkabroker (output)\nIf OK, you should get an output similar to :\n--- name: \"kafka\" status: \"UP\" details: resource: \"urn:kafka:cluster:id:KRzY-7iRTHy4d1UVyNlcuw\" brokers: - id: \"1\" host: \"localhost\" port: 9092 Create your first topics First, create a resource YAML file describing the topics you want to create on your cluster:\nfile: kafka-topics.yaml\napiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaTopicList\" items: - metadata: name: 'my-first-topic' spec: partitions: 5 replicationFactor: 1 configs: cleanup.policy: 'compact' - metadata: name: 'my-second-topic' spec: partitions: 4 replicationFactor: 1 configs: cleanup.policy: 'delete' Then, run the following Jikkou command to trigger the topic creation on the cluster:\njikkou create -f ./kafka-topics.yaml (output)\nTASK [ADD] Add topic 'my-first-topic' (partitions=5, replicas=-1, configs=[cleanup.policy=compact]) - CHANGED { \"changed\": true, \"end\": 1683986528117, \"resource\": { \"name\": \"my-first-topic\", \"partitions\": { \"after\": 5 }, \"replicas\": { \"after\": -1 }, \"configs\": { \"cleanup.policy\": { \"after\": \"compact\", \"operation\": \"ADD\" } }, \"operation\": \"ADD\" }, \"failed\": false, \"status\": \"CHANGED\" } TASK [ADD] Add topic 'my-second-topic' (partitions=4, replicas=-1, configs=[cleanup.policy=delete]) - CHANGED { \"changed\": true, \"end\": 1683986528117, \"resource\": { \"name\": \"my-second-topic\", \"partitions\": { \"after\": 4 }, \"replicas\": { \"after\": -1 }, \"configs\": { \"cleanup.policy\": { \"after\": \"delete\", \"operation\": \"ADD\" } }, \"operation\": \"ADD\" }, \"failed\": false, \"status\": \"CHANGED\" } EXECUTION in 772ms ok: 0, created: 2, altered: 0, deleted: 0 failed: 0 Tips In the above command, we chose to use the create command to create the new topics. But we could just as easily use the update or apply command to get the same result depending on our needs. Finally, you can verify that topics are created on the cluster\njikkou get kafkatopics --describe-default-configs Tips We use the --describe-default-configs to export built-in default configuration for configs that have a default value. Update Kafka Topics Edit your kafka-topics.yaml to add a retention.ms: 86400000 property to the defined topics.\nThen, run the following command.\njikkou update -f ./kafka-topics.yaml Delete Kafka Topics To delete all topics defines in the topics.yaml, add an annotation jikkou.io/delete: true as follows:\napiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaTopicList\" metadata: annotations: # Annotation to specify that all resources must be deleted. jikkou.io/delete: true items: - metadata: name: 'my-first-topic' spec: partitions: 5 replicationFactor: 1 configs: cleanup.policy: 'compact' - metadata: name: 'my-second-topic' spec: partitions: 4 replicationFactor: 1 configs: cleanup.policy: 'delete' Then, run the following command:\n$ jikkou apply \\ --files ./kafka-topics.yaml \\ --selector \"metadata.name MATCHES (my-.*-topic)\" \\ --dry-run Using the dry-run option, give you the possibility to check the changes that will be made before applying them.\nNow, rerun the above command without the --dry-run option to definitively delete the topics.\nRecommendation When working in a production environment, we strongly recommend running commands with a --selector option to ensure that changes are only applied to a specific set of resources. Also, always run your command in --dry-run mode to verify the changes that will be executed by Jikkou before continuing. Reading the Help To learn more about the available Jikkou commands, use jikkou help or type a command followed by the -h flag:\n$ jikkou help get Next Steps Now, you‚Äôre ready to use Jikkou!üöÄ\nAs next steps, we suggest reading the following documentation in this order:\nLearn Jikkou concepts Read the Developer Guide to understand how to use the Jikkou API for Java Look at the examples ","categories":"","description":"This guide covers how you can quickly get started using Jikkou.\n","excerpt":"This guide covers how you can quickly get started using Jikkou.\n","ref":"/jikkou/docs/tutorials/get_started/","tags":"","title":"Jikkou Getting Started"},{"body":" Welcome to the Jikkou documentation! Jikkou, means ‚Äúexecution (e.g. of a plan) or actual state (of things)‚Äù in Japanese.\nWhat is Jikkou ? Jikkou is a lightweight open-source tool designed to provide an efficient and easy way to manage, automate and provision resources on Event-Driven Data Mesh platforms (or, more simply, on any Apache Kafka Infrastructures).\nHow does Jikkou work ? Jikkou adopts a stateless approach and thus does not store any state internally. Instead, it leverages your platforms or services as the source of truth. This design allows you to seamlessly integrate Jikkou with other solutions (such as Ansible, Terraform, etc.) or use it on an ad hoc basis for specific needs, making it incredibly flexible and versatile.\nWhy Jikkou ? The Story Behind Jikkou. Jikkou was originally created as a side project to help development teams quickly recreate topics on Apache Kafka clusters used for testing purposes (the project was called Kafka Specs). At the time, the aim was to ensure that environments were always clean and ready to run integration tests.\nBut over time, new features were added, and so Jikkou was born with the aim to streamline day-to-day operations on Apache Kafka, ensuring that platform governance is no longer a tedious and boring task for both developers and administrators.\nToday, Jikkou is used in production on several projects, providing efficient management of Kafka resources through a GitOps approach. The project continues to evolve as an open-source project, as the solutions that have appeared over time in the Kafka ecosystem do not meet the needs of Kafka developers and administrators as well. In fact, existing solutions are either designed to work only with Kubernetes, or rely on dedicated services to manage the state of the solution.\nNow, we sincerely believe that Jikkou can play a role in bootstrapping a self-service platform for a Data Mesh organization, by unifying the way to manage the various assets required to create and manage a Data Product exposed through Apache Kafka.\nIs Jikkou for me ? Jikkou can be implemented regardless of the size of your team or data platform.\nSmall Development Team Jikkou is particularly useful for small development teams looking to quickly automate the creation and maintenance of their topics without having to implement a complex solution that requires learning a new technology or language.\nCentralized Infrastructure (DevOps) Team Jikkou can be very effective in larger contexts, where the configuration of your Kafka Topics, ACLs, and Quotas for all your data platform is managed by a single and centralized devops team.\nDecentralized Data Product Teams In an organization adopting Data Mesh principles, Jikkou can be leveraged in a decentralized way by each of your Data Teams to manage all the assets (e.g. Topics, ACLs, Schemas, Connectors, etc.) necessary to expose and manage their Data Products.\nCan I Use Jikkou with my Apache Kafka vendor ? Jikkou can be used any Apache Kafka infrastructures, including:\nApache Kafka Aiven Amazon MSK Confluent Cloud Redpanda ","categories":"","description":"What is Jikkou ?","excerpt":"What is Jikkou ?","ref":"/jikkou/docs/overview/","tags":"","title":"Overview"},{"body":" Jikkou Resources are entities that represent the state of a concrete instance of a concept that are part of the state of your system, like a Topic on an Apache Kafka cluster.\nResource Objects All resources can be distinguished between persistent objects, which are used to describe the desired state of your system, and transient objects, which are only used to enrich or provide additional capabilities for the definition of persistent objects.\nA resource is an object with a type (called a Kind) and a concrete model that describe the associated data. All resource are scoped by an API Group and Version.\nResource Definition Resources are described in YAML format.\nHere is a sample resource that described a Kafka Topic.\napiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopic metadata: name: 'my-topic' labels: environment: test annotations: {} spec: partitions: 1 replicas: 1 configs: min.insync.replicas: 1 cleanup.policy: 'delete' Resource Properties The following are the properties that can be set to describe a resource:\nProperty Description apiVersion The group/version of the resource type. kind The type of the describe resource. metadata.name An optional name to identify the resource. metadata.labels Arbitrary metadata to attach to the resource that can be handy when you have a lot of resources and you only need to identity or filters some objects. metadata.annotations Arbitrary non-identifying metadata to attach to the resource to mark them for a specific operation or to record some metadata. spec The object properties describing a desired state ","categories":"","description":"","excerpt":" Jikkou Resources are entities that represent the state of a concrete ‚Ä¶","ref":"/jikkou/docs/concepts/resource/","tags":["concept"],"title":"Resource"},{"body":" Here, you will find the list of core resources supported for Jikkou.\nCore Resources More information:\n","categories":"","description":"","excerpt":" Here, you will find the list of core resources supported for Jikkou. ‚Ä¶","ref":"/jikkou/docs/user-guide/core/resources/","tags":"","title":"Resources"},{"body":" Hands-on: Try the Jikkou: Get Started tutorials.\nConfiguration To set up the configuration settings used by Jikkou CLI, you will need create a jikkou config file, which is created automatically when you create a configuration context using:\njikkou config set-context \u003ccontext-name\u003e [--config-file=\u003cconfig-gile\u003e] [--config-props=\u003cconfig-value\u003e] By default, the configuration of jikkou is located under the path $HOME/.jikkou/config.\nThis jikkou config file defines all the contexts that can be used by jikkou CLI.\nFor example, below is the config file created during the Getting Started.\n{ \"currentContext\": \"localhost\", \"localhost\": { \"configFile\": null, \"configProps\": { \"kafka.client.bootstrap.servers\": \"localhost:9092\" } } } Most of the time, a context does not directly contain the configuration properties to be used, but rather points to a specific HOCON (Human-Optimized Config Object Notation) through the configFile property.\nThen, the configProps allows you to override some of the property define by this file.\nIn addition, if no configuration file path is specified, Jikkou will lookup for an application.conf to those following locations:\n./application.conf $HOME/.jikkou/application.conf Finally, Jikkou always fallback to a reference.conf file that you can use as a template to define your own configuration.\nreference.conf:\njikkou { # The paths from which to load extensions extension.paths = [${?JIKKOU_EXTENSION_PATH}] # Kafka Extension kafka { # The default Kafka Client configuration client { bootstrap.servers = \"localhost:9092\" bootstrap.servers = ${?JIKKOU_DEFAULT_KAFKA_BOOTSTRAP_SERVERS} } brokers { # If 'True' waitForEnabled = true waitForEnabled = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_ENABLED} # The minimal number of brokers that should be alive for the CLI stops waiting. waitForMinAvailable = 1 waitForMinAvailable = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_MIN_AVAILABLE} # The amount of time to wait before verifying that brokers are available. waitForRetryBackoffMs = 1000 waitForRetryBackoffMs = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_RETRY_BACKOFF_MS} # Wait until brokers are available or this timeout is reached. waitForTimeoutMs = 60000 waitForTimeoutMs = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_TIMEOUT_MS} } } schemaRegistry { url = \"http://localhost:8081\" url = ${?JIKKOU_DEFAULT_SCHEMA_REGISTRY_URL} } # The default custom transformations to apply on any resources. transformations = [] # The default custom validations to apply on any resources. validations = [ { name = \"topicMustHaveValidName\" type = io.streamthoughts.jikkou.kafka.validation.TopicNameRegexValidation priority = 100 config = { topicNameRegex = \"[a-zA-Z0-9\\\\._\\\\-]+\" topicNameRegex = ${?VALIDATION_DEFAULT_TOPIC_NAME_REGEX} } }, { name = \"topicMustHaveParitionsEqualsOrGreaterThanOne\" type = io.streamthoughts.jikkou.kafka.validation.TopicMinNumPartitionsValidation priority = 100 config = { topicMinNumPartitions = 1 topicMinNumPartitions = ${?VALIDATION_DEFAULT_TOPIC_MIN_NUM_PARTITIONS} } }, { name = \"topicMustHaveReplicasEqualsOrGreaterThanOne\" type = io.streamthoughts.jikkou.kafka.validation.TopicMinReplicationFactorValidation priority = 100 config = { topicMinReplicationFactor = 1 topicMinReplicationFactor = ${?VALIDATION_DEFAULT_TOPIC_MIN_REPLICATION_FACTOR} } } ] # The default custom reporters to report applied changes. reporters = [ # Uncomment following lines to enable default kafka reporter # { # name = \"default\" # type = io.streamthoughts.jikkou.kafka.reporter.KafkaChangeReporter # config = { # event.source = \"jikkou/cli\" # kafka = { # topic.creation.enabled = true # topic.creation.defaultReplicationFactor = 1 # topic.name = \"jikkou-resource-change-event\" # client = ${jikkou.kafka.client} { # client.id = \"jikkou-reporter-producer\" # } # } # } # } ] } Listing Contexts $ jikkou config get-contexts NAME localhost * development staging production Verify Current Context You can use jikkou config current-context command to show the context currently used by Jikkou CLI.\n$ jikkou config current-context Using context 'localhost' KEY VALUE ConfigFile ConfigProps {\"kafka.client.bootstrap.servers\": \"localhost:9092\"} Verify Current Configuration You can use jikkou config view command to show the configuration currently used by Jikkou CLI.\nTips To debug the configuration use by Jikkou, you can run the following command: jikkou config view --comments or jikkou config view --debug ","categories":"","description":"Learn how to configure Jikkou CLI.\n","excerpt":"Learn how to configure Jikkou CLI.\n","ref":"/jikkou/docs/jikkou-cli/cli-configuration/","tags":"","title":"CLI Configuration"},{"body":" Here, you will find the list of resources supported by the extension for Aiven.\nConfiguration You can configure the properties to be used to connect the Aiven service through the Jikkou client configuration property jikkou.aiven.\nExample:\njikkou { aiven { # Aiven project name project = \"http://localhost:8081\" # Aiven service name service = generic # URL to the Aiven REST API. apiUrl = \"https://api.aiven.io/v1/\" # Aiven Bearer Token. Tokens can be obtained from your Aiven profile page tokenAuth = null # Enable debug logging debugLoggingEnabled = false } } ","categories":"","description":"Learn how to configure the extensions for Aiven.\n","excerpt":"Learn how to configure the extensions for Aiven.\n","ref":"/jikkou/docs/user-guide/aiven/configuration/","tags":"","title":"Configuration"},{"body":" This section describes how to configure the Kafka Connect extension.\nExtension The Kafka Connect extension can be enabled/disabled via the configuration properties:\n# Example jikkou { extensions.provider.kafkaconnect.enabled = true } Configuration You can configure the properties to be used to connect the Kafka Connect cluster through the Jikkou client configuration property: jikkou.kafkaConnect.\nExample:\njikkou { kafkaConnect { # Array of Kafka Connect clusters configurations. clusters = [ { # Name of the cluster (e.g., dev, staging, production, etc.) name = \"locahost\" # URL of the Kafka Connect service url = \"http://localhost:8083\" # Method to use for authenticating on Kafka Connect. Available values are: [none, basicauth] authMethod = none # Use when 'authMethod' is 'basicauth' to specify the username for Authorization Basic header basicAuthUser = null # Use when 'authMethod' is 'basicauth' to specify the password for Authorization Basic header basicAuthPassword = null # Enable debug logging debugLoggingEnabled = false } ] } } ","categories":"","description":"Learn how to configure the extensions for Kafka Connect.\n","excerpt":"Learn how to configure the extensions for Kafka Connect.\n","ref":"/jikkou/docs/user-guide/kafka-connect/configuration/","tags":"","title":"Configuration"},{"body":" Here, you will find the list of resources supported for Apache Kafka.\nConfiguration The Apache Kafka extension is built on top of the Kafka Admin Client. You can configure the properties to be passed to kafka client through the Jikkou client configuration property jikkou.kafka.client.\nExample:\njikkou { kafka { client { bootstrap.servers = \"localhost:9092\" security.protocol = \"SSL\" ssl.keystore.location = \"/tmp/client.keystore.p12\" ssl.keystore.password = \"password\" ssl.keystore.type = \"PKCS12\" ssl.truststore.location = \"/tmp/client.truststore.jks\" ssl.truststore.password = \"password\" ssl.key.password = \"password\" } } } In addition, the extension support configuration settings to wait for at least a minimal number of brokers before processing.\njikkou { kafka { brokers { # If 'True' waitForEnabled = true waitForEnabled = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_ENABLED} # The minimal number of brokers that should be alive for the CLI stops waiting. waitForMinAvailable = 1 waitForMinAvailable = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_MIN_AVAILABLE} # The amount of time to wait before verifying that brokers are available. waitForRetryBackoffMs = 1000 waitForRetryBackoffMs = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_RETRY_BACKOFF_MS} # Wait until brokers are available or this timeout is reached. waitForTimeoutMs = 60000 waitForTimeoutMs = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_TIMEOUT_MS} } } } ","categories":"","description":"Learn how to configure the extensions for Apache Kafka.\n","excerpt":"Learn how to configure the extensions for Apache Kafka.\n","ref":"/jikkou/docs/user-guide/kafka/configuration/","tags":"","title":"Configuration"},{"body":" Here, you will find the list of resources supported for SchemaRegistry.\nConfiguration You can configure the properties to be used to connect the SchemaRegistry service through the Jikkou client configuration property jikkou.schemaRegistry.\nExample:\njikkou { schemaRegistry { # Comma-separated list of URLs for schema registry instances that can be used to register or look up schemas url = \"http://localhost:8081\" # The name of the schema registry implementation vendor - can be any value vendor = generic # Method to use for authenticating on Schema Registry. Available values are: [none, basicauth] authMethod = none # Use when 'schemaRegistry.authMethod' is 'basicauth' to specify the username for Authorization Basic header basicAuthUser = null # Use when 'schemaRegistry.authMethod' is 'basicauth' to specify the password for Authorization Basic header basicAuthPassword = null # Enable debug logging debugLoggingEnabled = false } } ","categories":"","description":"Learn how to configure the extensions for SchemaRegistry.\n","excerpt":"Learn how to configure the extensions for SchemaRegistry.\n","ref":"/jikkou/docs/user-guide/schema-registry/configuration/","tags":"","title":"Configuration"},{"body":" Jikkou can be installed either from source, or from releases.\nFrom SDKMan! (recommanded) The latest stable release of jikkou (x86) for Linux, and macOS can be retrieved via https://sdkman.io/[SDKMan]:\nsdk install jikkou From The Jikkou Project Releases Every release released versions of Jikkou is available:\nAs a zip/tar.gz package from GitHub Releases (for Linux, MacOS) As a fatJar available from Maven Central As a docker image available from Docker Hub. These are the official ways to get Jikkou releases that you manually downloaded and installed.\nInstall From Release distribution Download your desired version Unpack it (unzip jikkou-0.30.0-linux-x86_64.zip) Move the unpacked directory to the desired destination (mv jikkou-0.30.0-linux-x86_64 /opt/jikkou) Add the executable to your PATH (export PATH=$PATH:/opt/jikkou/bin) From there, you should be able to run the client: jikkou help.\nIt is recommended to install the bash/zsh completion script jikkou_completion:\nwget https://raw.githubusercontent.com/streamthoughts/jikkou/master/jikkou_completion . jikkou_completion or alternatively, run the following command for generation the completion script.\n$ source \u003c(jikkou generate-completion) Using Docker Image # Create a Jikkou configfile (i.e., jikkouconfig) cat \u003c\u003c EOF \u003ejikkouconfig { \"currentContext\" : \"localhost\", \"localhost\" : { \"configFile\" : null, \"configProps\" : { \"kafka.client.bootstrap.servers\" : \"localhost:9092\" } } } EOF # Run Docker docker run -it \\ --net host \\ --mount type=bind,source=\"$(pwd)\"/jikkouconfig,target=/etc/jikkou/config \\ streamthoughts/jikkou:latest -V Development Builds In addition to releases you can download or install development snapshots of Jikkou.\nFrom Docker Hub Docker images are built and push to Docker Hub from the latest main branch. They are not official releases, and may not be stable. However, they offer the opportunity to test the cutting edge features.\n$ docker run -it streamthoughts/jikkou:main From Source (Linux, macOS) Building Jikkou from source is slightly more work, but is the best way to go if you want to test the latest ( pre-release) Jikkou version.\nPrerequisites To build the project you will need:\nJava 17 (i.e. $JAVA_HOME environment variable is configured). GraalVM 22.1.0 or newer to create native executable TestContainer to run integration tests Create Native Executable # Build and run all tests ./mvnw clean verify -Pnative You can then execute the native executable with: ./jikkou-cli/target/jikkou-$PROJECT_VERSION-runner\nBuild Debian Package (.deb) # Build and run all tests ./mvnw clean package -Pnative ./mvnw package -Pdeb You can then install the package with: sudo dpkg -i ./dist/jikkou-$PROJECT_VERSION-linux-x86_64.deb\nNOTE: Jikkou will install itself in the directory : /opt/jikkou\nBuild RPM Package # Build and run all tests ./mvnw clean package -Pnative ./mvnw package -Prpm The RPM package will available in the ./target/rpm/jikkou/RPMS/noarch/ directory.\n","categories":"","description":"This guide shows how to install the Jikkou CLI.\n","excerpt":"This guide shows how to install the Jikkou CLI.\n","ref":"/jikkou/docs/install/","tags":"","title":"Install Jikkou"},{"body":" Jikkou is not only a CLI but also a Java library that you can use internally in your project.\nExamples Create Kafka Topics The example below shows how to use the JikkouApi to create Kafka Topics from resource description file.\nYou will need to add the following dependency in the pom.xml file of your project.\n\u003cdependency\u003e \u003cgroupId\u003eio.streamthoughts\u003c/groupId\u003e \u003cartifactId\u003ejikkou-extension-kafka\u003c/artifactId\u003e \u003cversion\u003e${jikkou.version}\u003c/version\u003e \u003c/dependency\u003e public class CreateKafkaTopicsExample { public static void main(String[] args) { // (1) Register Kinds ResourceDeserializer.registerKind(V1KafkaTopic.class); ResourceDeserializer.registerKind(V1KafkaTopicList.class); // (2) Load Resources HasItems resources = ResourceLoader.create().load(List.of(\"./kafka-topics.yaml\")); // (3) Create and configure Jikkou API AdminClientContext clientContext = new AdminClientContext( () -\u003e AdminClient.create(Map.of(\"bootstrap.servers\", \"localhost:9092\")) ); try (JikkouApi api = DefaultApi.builder() .withCollector(new AdminClientKafkaTopicCollector(clientContext)) .withController(new AdminClientKafkaTopicController(clientContext)) .build()) { // (4) Execute Reconciliation List\u003cChangeResult\u003cChange\u003e\u003e changes = api.apply( resources, ReconciliationMode.CREATE, ReconciliationContext.with(false) // dry-run ); // (5) Do something with changes System.out.println(changes); } } } ","categories":"","description":"Learn how to use the Jikkou programmatically\n","excerpt":"Learn how to use the Jikkou programmatically\n","ref":"/jikkou/docs/developer-guide/api/","tags":"","title":"JikkouApi"},{"body":"Labels You can use labels to attach arbitrary identifying metadata to objects.\nLabels are key/value maps:\nmetadata: labels: \"key1\": \"value-1\" \"key2\": \"value-2\" Note The keys in the map must be string, but values can be any scalar types (string, boolean, or numeric). Labels are not persistent Jikkou is completely stateless. In other words, it will not store any state about the describe resources objects. Thus, when retrieving objects from your system labels may not be reattached to the metadata objects. Example metadata: labels: environment: \"stating\" Annotations You can use annotations to attach arbitrary non-identifying metadata to objects.\nAnnotations are key/value maps:\nmetadata: annotations: key1: \"value-1\" key2: \"value-2\" Note The keys in the map must be string, but the values can be of any scalar types (string, boolean, or numeric). Built-in Annotations jikkou.io/ignore Used on: All Objects.\nThis annotation indicates whether the object should be ignored for reconciliation.\njikkou.io/bypass-validations Used on: All Objects.\nThis annotation indicates whether the object should bypass the validation chain. In other words, no validations will be applied on the object.\njikkou.io/delete Used on: All Objects.\nThis annotation indicates (when set to true) that the object should be deleted from your system.\njikkou.io/resource-location Used by jikkou.\nThis annotation is automatically added by Jikkou to an object when loaded from your local filesystem.\njikkou.io/items-count Used by jikkou.\nThis annotation is automatically added by Jikkou to an object collection grouping several resources of homogeneous type.\n","categories":"","description":"","excerpt":"Labels You can use labels to attach arbitrary identifying metadata to ‚Ä¶","ref":"/jikkou/docs/concepts/labels-and-annotations/","tags":"","title":"Labels and annotations"},{"body":" Here, you will find the list of resources supported by the extensions for Aiven.\nAiven for Apache Kafka Resources More information:\n","categories":"","description":"Learn how to use the built-in resources provided by the extensions for Aiven.\n","excerpt":"Learn how to use the built-in resources provided by the extensions for ‚Ä¶","ref":"/jikkou/docs/user-guide/aiven/resources/","tags":"","title":"Resources"},{"body":" Here, you will find the list of resources supported by the Kafka Connect Extension.\nKafka Connect Resources More information:\n","categories":"","description":"Learn how to use the resources provided by the Kafka Connect extension.\n","excerpt":"Learn how to use the resources provided by the Kafka Connect ‚Ä¶","ref":"/jikkou/docs/user-guide/kafka-connect/resources/","tags":"","title":"Resources"},{"body":" Here, you will find the list of resources supported for Apache Kafka.\nApache Kafka Resources More information:\n","categories":"","description":"Learn how to use the built-in resources provided by the extensions for Apache Kafka.\n","excerpt":"Learn how to use the built-in resources provided by the extensions for ‚Ä¶","ref":"/jikkou/docs/user-guide/kafka/resources/","tags":"","title":"Resources"},{"body":" Here, you will find the list of resources supported for Schema Registry.\nSchema Registry Resources More information:\n","categories":"","description":"Learn how to use the built-in resources provided by the extensions for Schema Registry.\n","excerpt":"Learn how to use the built-in resources provided by the extensions for ‚Ä¶","ref":"/jikkou/docs/user-guide/schema-registry/resources/","tags":"","title":"Resources"},{"body":" Here, you will find information to use the built-in transformations for Apache Kafka resources.\nMore information:\n","categories":"","description":"Learn how to use the built-in transformation provided by the extensions for Apache Kafka.\n","excerpt":"Learn how to use the built-in transformation provided by the ‚Ä¶","ref":"/jikkou/docs/user-guide/kafka/transformations/","tags":"","title":"Transformations"},{"body":" Try the tutorials for common Jikkou tasks and use cases.\n","categories":"","description":"Learn common Jikkou tasks and use cases.\n","excerpt":"Learn common Jikkou tasks and use cases.\n","ref":"/jikkou/docs/tutorials/","tags":"","title":"Jikkou Tutorials"},{"body":"","categories":"","description":"Learn automating Jikkou\n","excerpt":"Learn automating Jikkou\n","ref":"/jikkou/docs/jikkou-cli/automating/","tags":"","title":"Automating Jikkou"},{"body":"Setup Jikkou The streamthoughts/setup-jikkou action is a JavaScript action that sets up Jikkou in your GitHub Actions workflow by:\nDownloading a specific version of Jikkou CLI and adding it to the PATH. Configuring JIKKOU CLI with a custom configuration file. After you‚Äôve used the action, subsequent steps in the same job can run arbitrary Jikkou commands using the GitHub Actions run syntax. This allows most Jikkou commands to work exactly like they do on your local command line.\nUsage steps: - uses: streamthoughts/setup-jikkou@v1 A specific version of Jikkou CLI can be installed:\nsteps: - uses: streamthoughts/setup-jikkou@v0.1.0 with: jikkou_version: 0.29.0 A custom configuration file can be specified:\nsteps: - uses: streamthoughts/setup-jikkou@v0.1.0 with: jikkou_config: ./config/jikkouconfig.json Inputs This Action additionally supports the following inputs :\nProperty Default Description jikkou_version latest The version of Jikkou CLI to install. A value of latest will install the latest version of Jikkou CLI. jikkou_config The path to the Jikkou CLI config file. If set, Jikkou CLI will be configured through the JIKKOUCONFIG environment variable. ","categories":"","description":"Learn Jikkou Setup Github Action in your CI/CD Workflows\n","excerpt":"Learn Jikkou Setup Github Action in your CI/CD Workflows\n","ref":"/jikkou/docs/jikkou-cli/automating/githubactions/","tags":"","title":"Automate Jikkou with GitHub Actions"},{"body":" Hands-on: Try the Jikkou: Get Started tutorials.\n","categories":"","description":"Learn Jikkou's CLI-based workflows.\n","excerpt":"Learn Jikkou's CLI-based workflows.\n","ref":"/jikkou/docs/jikkou-cli/","tags":"","title":"Jikkou CLI Documentation"},{"body":" In the context of Jikkou, reconciliation refers to the process of comparing the desired state of an object with the actual state of the system and making any necessary corrections or adjustments to align them.\nChanges A Change represents a difference, detected during reconciliation, between two objects that can reconciled or corrected by adding, updating, or deleting an object or property attached to the actual state of the system.\nA Change represents a detected difference between two objects during the reconciliation process. These differences can be reconciled or corrected by adding, updating, or deleting an object or property associated with the actual state of the system\nJikkou identifies four types of changes:\nADD: Indicates the addition of a new object or property to an existing object.\nUPDATE: Indicates modifications made to an existing object or property of an existing object.\nDELETE: Indicates the removal of an existing object or property of an existing object.\nNONE: Indicates that no changes were made to an existing object or property.\nReconciliation Modes Depending on the chosen reconciliation mode, only specific types of changes will be applied.\nJikkou provides four distinct reconciliation modes that determine the types of changes to be applied:\nCREATE: This mode only applies changes that create new resource objects in your system. DELETE: This mode only applies changes that delete existing resource objects in your system. UPDATE: This mode only applies changes that create or update existing resource objects in your system. APPLY_ALL: This mode applies all changes to ensure that the actual state of a resource in the cluster matches the desired state defined in your resource definition file, regardless of the specific type of change. Each mode corresponds to a command offered by the Jikkou CLI (i.e., create, update, delete, and apply). Choose the appropriate mode based on your requirements.\nUsing JIKKOU CLI Some reconciliation modes might not be supported for all resources. Use jikkou extensions list --type Controller to check which actions could be perfomed for each resources. Reconciliation Options Depending on the type of resources being reconciled, the controller that will be involved in the reconciliation process might accept some options (i.e., using --options argument).\nMark Resource for Deletion To delete all the states associated with resource‚Äôs entities, you must add the following annotation to the resource definition:\nmetadata: annotations: jikkou.io/delete: true ","categories":"","description":"","excerpt":" In the context of Jikkou, reconciliation refers to the process of ‚Ä¶","ref":"/jikkou/docs/concepts/reconciliation/","tags":["concept"],"title":"Reconciliation"},{"body":" This section explains key concepts used within Jikkou:\n","categories":"","description":"Learn the differents concepts used within Jikkou\n","excerpt":"Learn the differents concepts used within Jikkou\n","ref":"/jikkou/docs/concepts/","tags":"","title":"Concepts"},{"body":" You can use selectors to select only a subset of resource objects to describe from your system or for which you want to perform a reconciliation process.\nField Selector (default) Jikkou provides the built-in FieldSelector that allows you to filter resource objects based on a field key.\nSelector Expression The expression below shows you how to select only resource having a label environement equals to either staging or production.\nmetadata.labels.environement IN (staging,production) Expression Operators Five kinds of operators are supported:\nIN NOTIN EXISTS MATCHES DOESNOTMATCH Using JIKKOU CLI Selectors can be specified via the Jikkou CLI option: --selector. ","categories":"","description":"","excerpt":" You can use selectors to select only a subset of resource objects to ‚Ä¶","ref":"/jikkou/docs/concepts/selectors/","tags":["concept","feature"],"title":"Selectors"},{"body":" Transformations are applied to inbound resources. Transformations are used to transform, enrich, or filter resource entities before they are validated and thus before the reconciliation process is executed on them.\nAvailable Transformations You can list all the available transformations using the Jikkou CLI command:\njikkou extensions list --type=Transformation [-kinds \u003ca resource kind to filter returned results\u003e] Transformation chain When using Jikkou CLI, you can configure a transformation chain that will be applied to every resource. This chain consists of multiple transformations, each designed to handle different types of resources. Jikkou ensures that a transformation is executed only for the resource types it supports. In cases where a resource is not accepted by a transformation, it is passed to the next transformation in the chain. This process continues until a suitable transformation is found or until all transformations have been attempted.\nConfiguration jikkou { # The list of transformations to execute transformations: [ { # Simple or fully qualified class name of the transformation extension. type = \"\" # Priority to be used for executing this transformation extension. # The lowest value has the highest priority, so it's run first. Minimum value is -2^31 (highest) and a maximum value is 2^31-1 (lowest). # Usually, values under 0 should be reserved for internal transformation extensions. priority = 0 config = { # Configuration properties for this transformation } } ] } Tips The config object of a Transformation always fallback on the top-level jikkou config. This allows you to globally declare some properties of the validation configuration. Example jikkou { # The list of transformations to execute transformations: [ { # Enforce a minimum number of replicas for a kafka topic type = KafkaTopicMinReplicasTransformation priority = 100 config = { minReplicationFactor = 4 } }, { # Enforce a {@code min.insync.replicas} for a kafka topic. type = KafkaTopicMinInSyncReplicasTransformation priority = 100 config = { minInSyncReplicas = 2 } } ] } ","categories":"","description":"","excerpt":" Transformations are applied to inbound resources. Transformations are ‚Ä¶","ref":"/jikkou/docs/concepts/transformations/","tags":["concept","feature","extension"],"title":"Transformations"},{"body":" The User Guide section helps you learn more about the functionalities of Jikkou.\n","categories":"","description":"Learn how to use Jikkou to provision resource configurations on your data infrastructure.\n","excerpt":"Learn how to use Jikkou to provision resource configurations on your ‚Ä¶","ref":"/jikkou/docs/user-guide/","tags":["how-to","docs"],"title":"User Guide"},{"body":" Here, you will find the necessary information to develop with the Jikkou API.\nMore information:\n","categories":"","description":"Learn how to use the Jikkou API\n","excerpt":"Learn how to use the Jikkou API\n","ref":"/jikkou/docs/developer-guide/","tags":"","title":"Developer Guide"},{"body":" Validations are applied to inbound resources to ensure that the resource entities adhere to specific rules or constraints. These validations are carried out after the execution of the transformation chain and before the reconciliation process takes place.\nAvailable Validations You can list all the available validations using the Jikkou CLI command:\njikkou extensions list --type=Validation [-kinds \u003ca resource kind to filter returned results\u003e] Validation chain When using Jikkou CLI, you can configure a validation chain that will be applied to every resource. This chain consists of multiple validations, each designed to handle different types of resources. Jikkou ensures that a validation is executed only for the resource types it supports. In cases where a resource is not accepted by a validation, it is passed to the next validation in the chain. This process continues until a suitable validation is found or until all validations have been attempted.\nConfiguration jikkou { # The list of validations to execute validations: [ { # Custom name for the validation rule name = \"\" # Simple or fully qualified class name of the validation extension. type = \"\" config = { # Configuration properties for this validation } } ] } Tips The config object of a Validation always fallback on the top-level jikkou config. This allows you to globally declare some properties of the validation configuration. Example jikkou { # The list of transformations to execute validations: [ { # Custom name for the validation rule name = topicMustBePrefixedWithRegion # Simple or fully qualified class name of the validation extension. type = TopicNameRegexValidation # The config values that will be passed to the validation. config = { topicNameRegex = \"(europe|northamerica|asiapacific)-.+\" } } ] } ","categories":"","description":"","excerpt":" Validations are applied to inbound resources to ensure that the ‚Ä¶","ref":"/jikkou/docs/concepts/validations/","tags":["concept","feature","extension"],"title":"Validations"},{"body":" Template helps you to dynamically define resource definition files from external data.\nTemplate Engine Jikkou provides a simple templating mechanism based-on Jinjava, a Jinja template engine for Java.\nRead the official documentation of Jinja to learn more about the syntax and semantics of the template engine.\nHow Does It Work ? Jikkou performs the rendering of your template in two phases:\nFirst, an initial rendering is performed using only the values and labels passed through the command-lines arguments. Thus, it is perfectly OK if your resource file is not initially a valid YAML file. Then, a second and final rendering is performed after parsing the YAML resource file using the additional values and labels as defined into the YAML resource file. Therefore, it‚Äôs important that your resource file is converted into a valid YAML file after the first rendering. Important You should use {% raw %}...{% endraw %} tags to ensure the variables defined into the template are not be interpreted during the first rendering. VariablesCLI-Configuration.md Jikkou defines a number of top-level variables that are passed to the template engine.\nvalues:\nThe values passed into the template through the command-line --values-files and/or --set-value arguments In addition, values can be defined into the application.conf file and directly into the template file using the property template.values. By default, values is empty. labels:\nThe labels passed into the template through the command-line argument: --set-label. In addition, labels can be defined into the template file using the property metadata.labels. By default, labels is empty. system.env:\nThis provides access to all environment variables. system.props:\nThis provides access to all system properties. Template Values When using templating, a resource definition file may contain the additional property template. fields:\napiVersion: The api version (required) kind: The resource kind (required) metadata: labels: The set of key/value pairs that you can use to describe your resource file (optional) annotations: The set of key/value pairs automatically generated by the tool (optional) template: values: The set of key/value pairs to be passed to the template engine (optional) spec: Specification of the resource Values Data File Values Data File are used to define all the necessary values (i.e., the variables) to be used for generating a template.\nExample # file: ./values.yaml topicConfigs: partitions: 4 replicas: 3 topicPrefix: \"{{ system.env.TOPIC_PREFIX | default('test', true) }}\" countryCodes: - fr - be - de - es - uk - us Template Resource File Example # file: ./kafka-topics.tpl apiVersion: 'kafka.jikkou.io/v1beta2' kind: 'KafkaTopicList' items: { % for country in values.countryCodes % } - metadata: name: \"{{ values.topicPrefix}}-iot-events-{{ country }}\" spec: partitions: { { values.topicConfigs.partitions } } replicas: { { values.topicConfigs.replicas } } configMapRefs: - TopicConfig { % endfor % } --- apiVersion: \"core.jikkou.io/v1beta2\" kind: \"ConfigMap\" metadata: name: TopicConfig template: values: default_min_insync_replicas: \"{{ values.topicConfigs.replicas | default(3, true) | int | add(-1) }}\" data: retention.ms: 3600000 max.message.bytes: 20971520 min.insync.replicas: '{% raw %}{{ values.default_min_insync_replicas }}{% endraw %}' Command\n$ TOPIC_PREFIX=local jikkou validate --files topics.tpl --values-files values.yaml (Output)\n--- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaTopicList\" metadata: labels: { } annotations: jikkou.io/resource-location: \"file:///tmp/jikkou/topics.tpl\" spec: topics: - metadata: name: \"local-iot-events-fr\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" retention.ms: 3600000 max.message.bytes: 20971520 - metadata: name: \"local-iot-events-be\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" retention.ms: 3600000 max.message.bytes: 20971520 - metadata: name: \"local-iot-events-de\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" max.message.bytes: 20971520 retention.ms: 3600000 - metadata: name: \"local-iot-events-es\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" max.message.bytes: 20971520 retention.ms: 3600000 - metadata: name: \"local-iot-events-uk\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" max.message.bytes: 20971520 retention.ms: 3600000 - metadata: name: \"local-iot-events-us\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" max.message.bytes: 20971520 retention.ms: 3600000 ","categories":"","description":"","excerpt":" Template helps you to dynamically define resource definition files ‚Ä¶","ref":"/jikkou/docs/concepts/template/","tags":["concept","feature"],"title":"Template"},{"body":" Collectors are used to collect and describe all entities that exist into your system for a specific resource type.\nAvailable Collectors You can list all the available collectors using the Jikkou CLI command:\njikkou extensions list --type=Collector [-kinds \u003ca resource kind to filter returned results\u003e] ","categories":"","description":"","excerpt":" Collectors are used to collect and describe all entities that exist ‚Ä¶","ref":"/jikkou/docs/concepts/collector/","tags":["concept","feature","extension"],"title":"Collectors"},{"body":" Controllers are used to compute and apply changes required to reconcile resources into a managed system.\nAvailable Controllers You can list all the available controllers using the Jikkou CLI command:\njikkou extensions list --type=Controller [-kinds \u003ca resource kind to filter returned results\u003e] ","categories":"","description":"","excerpt":" Controllers are used to compute and apply changes required to ‚Ä¶","ref":"/jikkou/docs/concepts/controller/","tags":["concept","feature","extension"],"title":"Controllers"},{"body":" The KafkaTopicAclEntry resources are used to manage the Access Control Lists in Aiven for Apache Kafka¬Æ. A KafkaTopicAclEntry resource defines the permission to be granted to a user for one or more kafka topics.\nKafkaTopicAclEntry Specification Here is the resource definition file for defining a KafkaTopicAclEntry.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"KafkaTopicAclEntry\" # The resource kind (required) metadata: labels: { } annotations: { } spec: permission: \u003c\u003e # The permission. Accepted values are: READ, WRITE, READWRITE, ADMIN username: \u003c\u003e # The username topic: \u003c\u003e # Topic name or glob pattern Example Here is a simple example that shows how to define a single ACL entry using the KafkaTopicAclEntry resource type.\nfile: kafka-topic-acl-entry.yaml\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"KafkaTopicAclEntry\" metadata: labels: { } annotations: { } spec: permission: \"READWRITE\" username: \"alice\" topic: \"public-*\" KafkaTopicAclEntryList If you need to define multiple ACL entries (e.g. using a template), it may be easier to use a KafkaTopicAclEntryList resource.\nSpecification Here the resource definition file for defining a KafkaTopicList.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"KafkaTopicAclEntryList\" # The resource kind (required) metadata: # (optional) name: \u003cThe name of the topic\u003e labels: { } annotations: { } items: [ ] # An array of KafkaTopicAclEntry Example Here is a simple example that shows how to define a single YAML file containing two ACL entry definitions using the KafkaTopicAclEntryList resource type.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"KafkaTopicAclEntryList\" items: - spec: permission: \"READWRITE\" username: \"alice\" topic: \"public-*\" - spec: permission: \"READ\" username: \"bob\" topic: \"public-*\" ","categories":"","description":"Learn how to manage Access Control Lists (ACLs) in Aiven for Apache Kafka¬Æ\n","excerpt":"Learn how to manage Access Control Lists (ACLs) in Aiven for Apache ‚Ä¶","ref":"/jikkou/docs/user-guide/aiven/resources/kafka-topic-acl/","tags":["feature","resources"],"title":"ACL for Aiven Apache Kafka¬Æ"},{"body":" Here, you will find information to use the Core extensions.\nMore information:\n","categories":"","description":"Core extensions for Jikkou\n","excerpt":"Core extensions for Jikkou\n","ref":"/jikkou/docs/user-guide/core/","tags":"","title":"Core"},{"body":"Prerequisites Jdk 17 (see https://sdkman.io/ for installing java locally) Git Docker and Docker-Compose Your favorite IDE Building Jikkou We use Maven Wrapper to build our project. The simplest way to get started is:\nFor building distribution files.\n$ ./mvnw clean package -Pdist -DskipTests Alternatively, we also use Make to package and build the Docker image for Jikkou:\n$ make Running tests For running all tests and checks:\n$ ./mvnw clean verify Code Format This project uses the Maven plugin Spotless to format all Java classes and to apply some code quality checks.\nBugs \u0026 Security This project uses the Maven plugin SpotBugs and FindSecBugs to run some static analysis to look for bugs in Java code.\nReported bugs can be analysed using SpotBugs GUI:\n$ ./mvnw spotbugs:gui ","categories":"","description":"How to set up your environment for developing on Jikkou.\n","excerpt":"How to set up your environment for developing on Jikkou.\n","ref":"/jikkou/docs/community/developer-guide/","tags":"","title":"Developer Guide"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/docs/","tags":"","title":"Documentation"},{"body":"Extensions Jikkou allows implementing and configuring extensions, i.e., Validation, and Transformer.\nJikkou‚Äôs sources are available onMaven Central\nFor Maven:\n\u003cdependency\u003e \u003cgroupId\u003eio.streamthoughts\u003c/groupId\u003e \u003cartifactId\u003ejikkou-api\u003c/artifactId\u003e \u003cversion\u003e${jikkou.version}\u003c/version\u003e \u003c/dependency\u003e For Gradle:\nimplementation group: 'io.streamthoughts', name: 'jikkou', version: ${jikkou.version} Packaging To make your extensions available to Jikkou, install them into one or many local directories. Then, use the jikkou.extension.paths property to configure the list of locations from which the extensions will be loaded.\nEach configured directories may contain:\nan uber JAR containing all the classes and third-party dependencies for the extensions. a directory containing all JARs for the extensions. ","categories":"","description":"","excerpt":"Extensions Jikkou allows implementing and configuring extensions, ‚Ä¶","ref":"/jikkou/docs/concepts/extensions/","tags":["feature","extensions"],"title":"Extensions"},{"body":" This section regroups all frequently asked questions about Jikkou.\n","categories":"","description":"","excerpt":" This section regroups all frequently asked questions about Jikkou.\n","ref":"/jikkou/docs/frequently-asked-questions/","tags":"","title":"Frequently Asked Questions"},{"body":" This section describes the resource definition format for KafkaConnector entities, which can be used to define the configuration and status of connectors you plan to create and manage on specific Kafka Connect clusters.\nDefinition Format of KafkaConnector Below is the overall structure of the KafkaConnector resource.\n--- apiVersion: \"kafka.jikkou.io/v1beta1\" # The api version (required) kind: \"KafkaConnector\" # The resource kind (required) metadata: name: \u003cstring\u003e # The name of the connector (required) labels: # Name of the Kafka Connect cluster to create the connector instance in (required). kafka.jikkou.io/connect-cluster: \u003cstring\u003e annotations: { } spec: connectorClass: \u003cstring\u003e # Name or alias of the class for this connector. tasksMax: \u003cinteger\u003e # The maximum number of tasks for the Kafka Connector. config: # Configuration properties of the connector. \u003ckey\u003e: \u003cvalue\u003e state: \u003cstring\u003e # The state the connector should be in. Defaults to running. See below for details about all these fields.\nMetadata metadata.name [required] The name of the connector.\nlabels.kafka.jikkou.io/connect-cluster [required] the name of the Kafka Connect cluster to create the connector instance in. The cluster name must be configured through the kafkaConnect.clusters[] Jikkou‚Äôs configuration setting (see: Configuration).\nSpecification spec.connectorClass [required] The name or alias of the class for this connector.\nspec.tasksMax [optional] The maximum number of tasks for the Kafka Connector. Default is 1.\nspec.config [required] The connector‚Äôs configuration properties.\nspec.state [optional] The state the connector should be in. Defaults to running.\nBelow are the valid values:\nrunning: Transition the connector and its tasks to RUNNING state. paused: Pause the connector and its tasks, which stops message processing until the connector is resumed. stopped: Completely shut down the connector and its tasks. The connector config remains present in the config topic of the cluster (if running in distributed mode), unmodified. Examples The following is an example of a resource describing a Kafka connector:\n--- # Example: file: kafka-connector-filestream-sink.yaml apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: name: \"local-file-sink\" labels: kafka.jikkou.io/connect-cluster: \"my-connect-cluster\" spec: connectorClass: \"FileStreamSink\" tasksMax: 1 config: file: \"/tmp/test.sink.txt\" topics: \"connect-test\" state: \"RUNNING\" Listing KafkaConnector You can retrieve the state of Kafka Connector instances running on your Kafka Connect clusters using the jikkou get kafkaconnectors (or jikkou get kc) command.\nUsage $jikkou get kc --help Usage: Get all 'KafkaConnector' resources. jikkou get kafkaconnectors [-hV] [--expand-status] [-o=\u003cformat\u003e] [-s=\u003cexpressions\u003e]... Description: Use jikkou get kafkaconnectors when you want to describe the state of all resources of type 'KafkaConnector'. Options: --expand-status Retrieves additional information about the status of the connector and its tasks. -h, --help Show this help message and exit. -o, --output=\u003cformat\u003e Prints the output in the specified format. Allowed values: json, yaml (default yaml). -s, --selector=\u003cexpressions\u003e The selector expression use for including or excluding resources. -V, --version Print version information and exit. (The output from your current Jikkou version may be different from the above example.)\nExamples (command)\n$ jikkou get kc --expand-status (output)\napiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: name: \"local-file-sink\" labels: kafka.jikkou.io/connect-cluster: \"localhost\" spec: connectorClass: \"FileStreamSink\" tasksMax: 1 config: file: \"/tmp/test.sink.txt\" topics: \"connect-test\" state: \"RUNNING\" status: connectorStatus: name: \"local-file-sink\" connector: state: \"RUNNING\" worker_id: \"localhost:8083\" tasks: id: 1 state: \"RUNNING\" worker_id: \"localhost:8083\" The status.connectorStatus provides the connector status, as reported by the Kafka Connect REST API.\n","categories":"","description":"Learn how to manage Kafka Connectors.\n","excerpt":"Learn how to manage Kafka Connectors.\n","ref":"/jikkou/docs/user-guide/kafka-connect/resources/connector/","tags":["feature","resources"],"title":"KafkaConnectors"},{"body":" SchemaRegistrySubject resources are used to define the subject schemas you want to manage on your SchemaRegistry. A SchemaRegistrySubject resource defines the schema, the compatibility level, and the references to be associated with a subject version.\nSchemaRegistrySubject Specification Here is the resource definition file for defining a SchemaRegistrySubject.\napiVersion: \"schemaregistry.jikkou.io/v1beta2\" # The api version (required) kind: \"SchemaRegistrySubject\" # The resource kind (required) metadata: name: \u003cThe name of the subject\u003e # (required) labels: { } annotations: { } spec: schemaRegistry: vendor: \u003cvendor_name\u003e # (optional) The vendor of the SchemaRegistry, e.g., Confluent, Karapace, etc compatibilityLevel: \u003ccompatibility_level\u003e # (optional) The schema compatibility level for this subject. schemaType: \u003cThe schema format\u003e # (required) Accepted values are: AVRO, PROTOBUF, JSON schema: $ref: \u003curl or path\u003e # references: # Specifies the names of referenced schemas (optional array). - name: \u003c\u003e # The name for the reference. subject: \u003c\u003e # The subject under which the referenced schema is registered. version: \u003c\u003e # The exact version of the schema under the registered subject. ] The metadata.name property is mandatory and specifies the name of the Subject.\nTo use the SchemaRegistry default values for the compatibilityLevel you can omit the property.\nExample Here is a simple example that shows how to define a single subject AVRO schema for type using the SchemaRegistrySubject resource type.\nfile: subject-user.yaml\n--- apiVersion: \"schemaregistry.jikkou.io/v1beta2\" kind: \"SchemaRegistrySubject\" metadata: name: \"User\" labels: { } annotations: schemaregistry.jikkou.io/normalize-schema: true spec: compatibilityLevel: \"FULL_TRANSITIVE\" schemaType: \"AVRO\" schema: $ref: ./user-schema.avsc file: user-schema.avsc\n--- { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"name\", \"type\": [ \"null\", \"string\" ], \"default\": null, }, { \"name\": \"favorite_number\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"favorite_color\", \"type\": [ \"null\", \"string\" ], \"default\": null } ] } Alternatively, we can directly pass the Avro schema as follows :\nfile: subject-user.yaml\n--- apiVersion: \"schemaregistry.jikkou.io/v1beta2\" kind: \"SchemaRegistrySubject\" metadata: name: \"User\" labels: { } annotations: schemaregistry.jikkou.io/normalize-schema: true spec: compatibilityLevel: \"FULL_TRANSITIVE\" schemaType: \"AVRO\" schema: | { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"name\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"favorite_number\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"favorite_color\", \"type\": [ \"null\", \"string\"], \"default\": null } ] } ","categories":"","description":"Learn how to manage SchemaRegistry Subjects.\n","excerpt":"Learn how to manage SchemaRegistry Subjects.\n","ref":"/jikkou/docs/user-guide/schema-registry/resources/subject/","tags":["feature","resources"],"title":"Schema Registry Subjects"},{"body":" This section describes the resource definition format for KafkaConsumerGroup entities, which can be used to define the consumer groups you plan to manage on a specific Kafka cluster.\nListing KafkaConsumerGroup You can retrieve the state of Kafka Consumer Groups using the jikkou get kafkaconsumergroups (or jikkou get kcg) command.\nUsage $ jikkou get kafkaconsumergroups --help Usage: Get all 'KafkaConsumerGroup' resources. jikkou get kafkaconsumergroups [-hV] [--list] [--offsets] [--logger-level=\u003clevel\u003e] [-o=\u003cformat\u003e] [--in-states=PARAM]... [-s=\u003cexpressions\u003e]... DESCRIPTION: Use jikkou get kafkaconsumergroups when you want to describe the state of all resources of type 'KafkaConsumerGroup'. OPTIONS: -h, --help Show this help message and exit. --in-states=PARAM If states is set, only groups in these states will be returned. Otherwise, all groups are returned. This operation is supported by brokers with version 2.6.0 or later --list Get resources as ResourceListObject. --logger-level=\u003clevel\u003e Specify the log level verbosity to be used while running a command. Valid level values are: TRACE, DEBUG, INFO, WARN, ERROR. For example, `--logger-level=INFO -o, --output=\u003cformat\u003e Prints the output in the specified format. Allowed values: json, yaml (default yaml). --offsets Specify whether consumer group offsets should be described. -s, --selector=\u003cexpressions\u003e The selector expression used for including or excluding resources. -V, --version Print version information and exit (The output from your current Jikkou version may be different from the above example.)\nExamples (command)\n$ jikkou get kafkaconsumergroups --in-states STABLE --offsets (output)\n--- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConsumerGroup\" metadata: name: \"my-group\" labels: kafka.jikkou.io/is-simple-consumer: false spec: state: \"STABLE\" members: - memberId: \"console-consumer-b103994e-bcd5-4236-9d03-97065057e594\" clientId: \"console-consumer\" host: \"/127.0.0.1\" assignments: - \"my-topic-0\" offsets: - topic: \"my-topic\" partition: 0 offset: 0 coordinator: id: \"101\" host: \"localhost\" port: 9092 ","categories":"","description":"Learn how to manage Kafka Consumer Groups.\n","excerpt":"Learn how to manage Kafka Consumer Groups.\n","ref":"/jikkou/docs/user-guide/kafka/resources/consumer_groups/","tags":["feature","resources"],"title":"Kafka Consumer Groups"},{"body":" KafkaTopic resources are used to define the topics you want to manage on your Kafka Cluster(s). A KafkaTopic resource defines the number of partitions, the replication factor, and the configuration properties to be associated to a topics.\nKafkaTopic Specification Here is the resource definition file for defining a KafkaTopic.\napiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaTopic\" # The resource kind (required) metadata: name: \u003cThe name of the topic\u003e # (required) labels: { } annotations: { } spec: partitions: \u003cNumber of partitions\u003e # (optional) replicas: \u003cNumber of replicas\u003e # (optional) configs: \u003cconfig_key\u003e: \u003cConfig Value\u003e # The topic config properties keyed by name to override (optional) configMapRefs: [ ] # The list of ConfigMap to be applied to this topic (optional) The metadata.name property is mandatory and specifies the name of the kafka topic.\nTo use the cluster default values for the number of partitions and replicas you can set the property spec.partitions and spec.replicas to -1.\nExample Here is a simple example that shows how to define a single YAML file containing two topic definition using the KafkaTopic resource type.\nfile: kafka-topics.yaml\n--- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopic metadata: name: 'my-topic-p1-r1' # Name of the topic labels: environment: example spec: partitions: 1 # Number of topic partitions (use -1 to use cluster default) replicas: 1 # Replication factor per partition (use -1 to use cluster default) configs: min.insync.replicas: 1 cleanup.policy: 'delete' --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopic metadata: name: 'my-topic-p2-r1' # Name of the topic labels: environment: example spec: partitions: 2 # Number of topic partitions (use -1 to use cluster default) replicas: 1 # Replication factor per partition (use -1 to use cluster default) configs: min.insync.replicas: 1 cleanup.policy: 'delete' See official Apache Kafka documentation for details about the topic-level configs.\nTips: Multiple topics can be included in the same YAML file by using --- lines. KafkaTopicList If you need to define multiple topics (e.g. using a template), it may be easier to use a KafkaTopicList resource.\nSpecification Here the resource definition file for defining a KafkaTopicList.\napiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaTopicList\" # The resource kind (required) metadata: # (optional) labels: { } annotations: { } items: [ ] # An array of KafkaTopic Example Here is a simple example that shows how to define a single YAML file containing two topic definitions using the KafkaTopicList resource type. In addition, the example uses a ConfigMap object to define the topic configuration only once.\nfile: kafka-topics.yaml\n--- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopicList metadata: labels: environment: example items: - metadata: name: 'my-topic-p1-r1' spec: partitions: 1 replicas: 1 configMapRefs: [ \"TopicConfig\" ] - metadata: name: 'my-topic-p2-r1' spec: partitions: 2 replicas: 1 configMapRefs: [ \"TopicConfig\" ] --- apiVersion: \"core.jikkou.io/v1beta2\" kind: ConfigMap metadata: name: 'TopicConfig' data: min.insync.replicas: 1 cleanup.policy: 'delete' ","categories":"","description":"Learn how to manage Kafka Topics.\n","excerpt":"Learn how to manage Kafka Topics.\n","ref":"/jikkou/docs/user-guide/kafka/resources/topics/","tags":["feature","resources"],"title":"Kafka Topics"},{"body":" Reporters can be used to report changes applied by Jikkou to a third-party system.\nConfiguration Jikkou allows you to configure multiple reporters as follows:\njikkou { # The list of reporters to execute reporters: [ { # Custom name for the reporter name = \"\" # Simple or fully qualified class name of the transformation extension. type = \"\" config = { # Configuration properties for this reporter } } ] } Tips The config object passed to a reporter will fallback on the top-level jikkou config. This allows you to globally declare some configuration settings. Built-in implementations Jikkou packs with some built-in ChangeReporter implementations:\nKafkaChangeReporter The KafkaChangeReporter can be used to send change results into a given kafka topic. Changes will be published as Cloud Events.\nConfiguration The below example shows how to configure the KafkaChangeReporter.\njikkou { # The default custom reporters to report applied changes. reporters = [ { name = \"kafka-reporter\" type = io.streamthoughts.jikkou.kafka.reporter.KafkaChangeReporter config = { # The 'source' of the event that will be generated. event.source = \"jikkou/cli\" kafka = { # If 'true', topic will be automatically created if it does not already exist. topic.creation.enabled = true # The default replication factor used for creating topic. topic.creation.defaultReplicationFactor = 1 # The name of the topic the events will be sent. topic.name = \"jikkou-resource-change-event\" # The configuration settings for Kafka Producer and AdminClient client = ${jikkou.kafka.client} { client.id = \"jikkou-reporter-producer\" } } } } ] } ","categories":"","description":"","excerpt":" Reporters can be used to report changes applied by Jikkou to a ‚Ä¶","ref":"/jikkou/docs/concepts/reporters/","tags":["feature","extensions"],"title":"Reporters"},{"body":" Here, you will find information to use the Apache Kafka extensions.\nMore information:\n","categories":"","description":"Extensions for Apache Kafka.\n","excerpt":"Extensions for Apache Kafka.\n","ref":"/jikkou/docs/user-guide/kafka/","tags":"","title":"Apache Kafka"},{"body":" KafkaPrincipalAuthorization resources are used to define Access Control Lists (ACLs) for principals authenticated to your Kafka Cluster.\nJikkou can be used to describe all ACL policies that need to be created on Kafka Cluster\nKafkaPrincipalAuthorization Specification --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalAuthorization\" metadata: name: \"User:Alice\" spec: roles: [ ] # List of roles to be added to the principal (optional) acls: # List of KafkaPrincipalACL (required) - resource: type: \u003cThe type of the resource\u003e # (required) pattern: \u003cThe pattern to be used for matching resources\u003e # (required) patternType: \u003cThe pattern type\u003e # (required) type: \u003cThe type of this ACL\u003e # ALLOW or DENY (required) operations: [ ] # Operation that will be allowed or denied (required) host: \u003cHOST\u003e # IP address from which principal will have access or will be denied (optional) For more information on how to define authorization and ACLs, see the official Apache Kafka documentation: Security\nOperations The list below describes the valid values for the spec.acls.[].operations property :\nREAD WRITE CERATE DELETE ALTER DESCRIBE CLUSTER_ACTION DESCRIBE_CONFIGS ALTER_CONFIGS IDEMPOTENT_WRITE CREATE_TOKEN DESCRIBE_TOKENS ALL For more information see official Apache Kafka documentation: Operations in Kafka\nResource Types The list below describes the valid values for the spec.acls.[].resource.type property :\nTOPIC GROUP CLUSTER USER TRANSACTIONAL_ID For more information see official Apache Kafka documentation: Resources in Kafka\nPattern Types The list below describes the valid values for the spec.acls.[].resource.patternType property :\nLITERAL: Use to allow or denied a principal to have access to a specific resource name. MATCH: Use to allow or denied a principal to have access to all resources matching the given regex. PREFIXED: Use to allow or denied a principal to have access to all resources having the given prefix. Example --- apiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaPrincipalAuthorization\" # The resource kind (required) metadata: name: \"User:Alice\" spec: acls: - resource: type: 'topic' pattern: 'my-topic-' patternType: 'PREFIXED' type: \"ALLOW\" operations: [ 'READ', 'WRITE' ] host: \"*\" - resource: type: 'topic' pattern: 'my-other-topic-.*' patternType: 'MATCH' type: 'ALLOW' operations: [ 'READ' ] host: \"*\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalAuthorization\" metadata: name: \"User:Bob\" spec: acls: - resource: type: 'topic' pattern: 'my-topic-' patternType: 'PREFIXED' type: 'ALLOW' operations: [ 'READ', 'WRITE' ] host: \"*\" KafkaPrincipalRole Specification apiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaPrincipalRole\" # The resource kind (required) metadata: name: \u003cName of role\u003e # The name of the role (required) spec: acls: [ ] # A list of KafkaPrincipalACL (required) Example --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalRole\" metadata: name: \"KafkaTopicPublicRead\" spec: acls: - type: \"ALLOW\" operations: [ 'READ' ] resource: type: 'topic' pattern: '/public-([.-])*/' patternType: 'MATCH' host: \"*\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalRole\" metadata: name: \"KafkaTopicPublicWrite\" spec: acls: - type: \"ALLOW\" operations: [ 'WRITE' ] resource: type: 'topic' pattern: '/public-([.-])*/' patternType: 'MATCH' host: \"*\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalAuthorization\" metadata: name: \"User:Alice\" spec: roles: - \"KafkaTopicPublicRead\" - \"KafkaTopicPublicWrite\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalAuthorization\" metadata: name: \"User:Bob\" spec: roles: - \"KafkaTopicPublicRead\" ","categories":"","description":"Learn how to manage Kafka Authorizations and ACLs. \n","excerpt":"Learn how to manage Kafka Authorizations and ACLs. \n","ref":"/jikkou/docs/user-guide/kafka/resources/acls/","tags":["feature","resources"],"title":"Kafka Authorizations"},{"body":" ","categories":"","description":"What does your user need to know to try your project?\n","excerpt":"What does your user need to know to try your project?\n","ref":"/jikkou/docs/community/","tags":"","title":"Community"},{"body":"Jikkou is an open source project, and we love getting patches and contributions to make Jikkou and its docs even better.\nContributing to Jikkou The Jikkou project itself lives in https://github.com/streamthoughts/jikkou\nCode reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.\nCreating issues Alternatively, if there‚Äôs something you‚Äôd like to see in Jikkou (or if you‚Äôve found something that isn‚Äôt working the way you‚Äôd expect), but you‚Äôre not sure how to fix it yourself, please create an issue.\n","categories":"","description":"How to contribute to Jikkou\n","excerpt":"How to contribute to Jikkou\n","ref":"/jikkou/docs/community/contribution-guidelines/","tags":"","title":"Contribution Guidelines"},{"body":" The KafkaQuota resources are used to manage the Quotas in Aiven for Apache Kafka¬Æ service. For more details, see https://docs.aiven.io/docs/products/kafka/concepts/kafka-quotas\nKafkaQuota Specification Here is the resource definition file for defining a KafkaQuota.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"KafkaQuota\" # The resource kind (required) metadata: labels: { } annotations: { } spec: user: \u003cstring\u003e # The username: (Optional: 'default' if null) clientId: \u003cstring\u003e # The client-id consumerByteRate: \u003cnumber\u003e # The quota in bytes for restricting data consumption producerByteRate: \u003cnumber\u003e # The quota in bytes for restricting data production requestPercentage: \u003cnumber\u003e Example Here is a simple example that shows how to define a single ACL entry using the KafkaQuota resource type.\nfile: kafka-quotas.yaml\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"KafkaQuota\" spec: user: \"default\" clientId: \"default\" consumerByteRate: 1048576 producerByteRate: 1048576 requestPercentage: 25 KafkaQuotaList If you need to define multiple Kafka quotas (e.g. using a template), it may be easier to use a KafkaQuotaList resource.\nSpecification Here the resource definition file for defining a KafkaTopicList.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"KafkaQuotaList\" # The resource kind (required) metadata: # (optional) labels: { } annotations: { } items: [ ] # An array of KafkaQuotaList Example Here is a simple example that shows how to define a single YAML file containing two ACL entry definitions using the KafkaQuotaList resource type.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"KafkaQuotaList\" items: - spec: user: \"default\" clientId: \"default\" consumerByteRate: 1048576 producerByteRate: 1048576 requestPercentage: 5 - spec: user: \"avnadmin\" consumerByteRate: 5242880 producerByteRate: 5242880 requestPercentage: 25 ","categories":"","description":"Learn how to manage Quotas in Aiven for Apache Kafka¬Æ\n","excerpt":"Learn how to manage Quotas in Aiven for Apache Kafka¬Æ\n","ref":"/jikkou/docs/user-guide/aiven/resources/kafka-quota/","tags":["feature","resources"],"title":"Quotas for Aiven Apache Kafka¬Æ"},{"body":" The SchemaRegistryAclEntry resources are used to manage the Access Control Lists in Aiven for Schema Registry. A SchemaRegistryAclEntry resource defines the permission to be granted to a user for one or more Schema Registry Subjects.\nSchemaRegistryAclEntry Specification Here is the resource definition file for defining a SchemaRegistryAclEntry.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"SchemaRegistryAclEntry\" # The resource kind (required) metadata: labels: { } annotations: { } spec: permission: \u003c\u003e # The permission. Accepted values are: READ, WRITE username: \u003c\u003e # The username resource: \u003c\u003e # The Schema Registry ACL entry resource name pattern NOTE: The resource name pattern should be Config: or Subject:\u003csubject_name\u003e where subject_name must consist of alpha-numeric characters, underscores, dashes, dots and glob characters * and ?.\nExample Here is an example that shows how to define a simple ACL entry using the SchemaRegistryAclEntry resource type.\nfile: schema-registry-acl-entry.yaml\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"SchemaRegistryAclEntry\" spec: permission: \"READ\" username: \"Alice\" resource: \"Subject:*\" SchemaRegistryAclEntryList If you need to define multiple ACL entries (e.g. using a template), it may be easier to use a SchemaRegistryAclEntryList resource.\nSpecification Here the resource definition file for defining a SchemaRegistryAclEntryList.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"SchemaRegistryAclEntryList\" # The resource kind (required) metadata: # (optional) labels: { } annotations: { } items: [ ] # An array of SchemaRegistryAclEntry Example Here is a simple example that shows how to define a single YAML file containing two ACL entry definitions using the SchemaRegistryAclEntryList resource type.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"SchemaRegistryAclEntryList\" items: - spec: permission: \"READ\" username: \"alice\" resource: \"Config:\" - spec: permission: \"WRITE\" username: \"alice\" resource: \"Subject:*\" ","categories":"","description":"Learn how to manage Access Control Lists (ACLs) in Aiven for Schema Registry\n","excerpt":"Learn how to manage Access Control Lists (ACLs) in Aiven for Schema ‚Ä¶","ref":"/jikkou/docs/user-guide/aiven/resources/schema-registry-acl/","tags":["feature","resources"],"title":"ACL for Aiven Schema Registry"},{"body":" Here, you will find information to use the Kafka Connect extension for Jikkou.\nMore information:\n","categories":"","description":"Integration between Apache Kafka Connect and Jikkou.\n","excerpt":"Integration between Apache Kafka Connect and Jikkou.\n","ref":"/jikkou/docs/user-guide/kafka-connect/","tags":"","title":"Kafka Connect"},{"body":" KafkaClientQuota resources are used to define the quota limits to be applied on Kafka consumers and producers. A KafkaClientQuota resource can be used to apply limit to consumers and/or producers identified by a client-id or a user principal.\nKafkaClientQuota Specification Here is the resource definition file for defining a KafkaClientQuota.\napiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaClientQuota\" # The resource kind (required) metadata: # (optional) labels: { } annotations: { } spec: type: \u003cThe quota type\u003e # (required) entity: clientId: \u003cThe id of the client\u003e # (required depending on the quota type). user: \u003cThe principal of the user\u003e # (required depending on the quota type). configs: requestPercentage: \u003cThe quota in percentage (%) of total requests\u003e # (optional) producerByteRate: \u003cThe quota in bytes for restricting data production\u003e # (optional) consumerByteRate: \u003cThe quota in bytes for restricting data consumption\u003e # (optional) Quota Types The list below describes the supported quota types:\nUSERS_DEFAULT: Set default quotas for all users. USER: Set quotas for a specific user principal. USER_CLIENT: Set quotas for a specific user principal and a specific client-id. USER_ALL_CLIENTS: Set default quotas for a specific user and all clients. CLIENT: Set default quotas for a specific client. CLIENTS_DEFAULT: Set default quotas for all clients. Example Here is a simple example that shows how to define a single YAML file containing two quota definitions using the KafkaClientQuota resource type.\nfile: kafka-quotas.yaml\n--- apiVersion: 'kafka.jikkou.io/v1beta2' kind: 'KafkaClientQuota' metadata: labels: { } annotations: { } spec: type: 'CLIENT' entity: clientId: 'my-client' configs: requestPercentage: 10 producerByteRate: 1024 consumerByteRate: 1024 --- apiVersion: 'kafka.jikkou.io/v1beta2' kind: 'KafkaClientQuota' metadata: labels: { } annotations: { } spec: type: 'USER' entity: user: 'my-user' configs: requestPercentage: 10 producerByteRate: 1024 consumerByteRate: 1024 KafkaClientQuotaList If you need to define multiple topics (e.g. using a template), it may be easier to use a KafkaClientQuotaList resource.\nSpecification Here the resource definition file for defining a KafkaTopicList.\napiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaClientQuotaList\" # The resource kind (required) metadata: # (optional) name: \u003cThe name of the topic\u003e labels: { } annotations: { } items: [ ] # An array of KafkaClientQuota Example Here is a simple example that shows how to define a single YAML file containing two KafkaClientQuota definition using the KafkaClientQuotaList resource type.\napiVersion: 'kafka.jikkou.io/v1beta2' kind: 'KafkaClientQuotaList' metadata: labels: { } annotations: { } items: - spec: type: 'CLIENT' entity: clientId: 'my-client' configs: requestPercentage: 10 producerByteRate: 1024 consumerByteRate: 1024 - spec: type: 'USER' entity: user: 'my-user' configs: requestPercentage: 10 producerByteRate: 1024 consumerByteRate: 1024 ","categories":"","description":"Learn how to manage Kafka Client Quotas\n","excerpt":"Learn how to manage Kafka Client Quotas\n","ref":"/jikkou/docs/user-guide/kafka/resources/quotas/","tags":["feature","resources"],"title":"Kafka Quotas"},{"body":" A KafkaTableRecord resource can be used to produce a key/value record into a given compacted topic, i.e., a topic with cleanup.policy=compact (a.k.a. KTable).\nKafkaTableRecord Specification Here is the resource definition file for defining a KafkaTableRecord.\napiVersion: \"kafka.jikkou.io/v1beta1\" # The api version (required) kind: \"KafkaTableRecord\" # The resource kind (required) metadata: name: \u003cstring\u003e # The topic name (required) labels: { } annotations: { } spec: headers: # The list of headers - name: \u003cstring\u003e value: \u003cstring\u003e key: # The record-key (required) type: \u003cstring\u003e # The record-key type. Must be one of: BINARY, STRING, JSON (required) data: # The record-key in JSON serialized form. $ref: \u003curl or path\u003e # Or an url to a local file containing the JSON string value. value: # The record-value (required) type: \u003cstring\u003e # The record-value type. Must be one of: BINARY, STRING, JSON (required) data: # The record-value in JSON serialized form. $ref: \u003curl or path\u003e # Or an url to a local file containing the JSON string value. Usage The KafkaTableRecord resource has been designed primarily to manage reference data published and shared via Kafka. Therefore, it is highly recommended to use this resource only with compacted Kafka topics containing a small amount of data.\nExamples Here are some examples that show how to a KafkaTableRecord using the different supported data type.\nSTRING:\n--- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaTableRecord\" metadata: # The name of the kafka table topic. name: \"my-topic\" spec: headers: - name: \"content-type\" value: \"application/text\" key: type: STRING data: | \"bar\" value: type: STRING data: | \"foo\" JSON:\n--- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaTableRecord\" metadata: # The name of the kafka table topic. name: \"my-topic\" spec: headers: - name: \"content-type\" value: \"application/text\" key: type: STRING data: | \"bar\" value: type: JSON data: | { \"foo\": \"bar\" } BINARY:\n--- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaTableRecord\" metadata: # The name of the kafka table topic. name: \"my-topic\" spec: headers: - name: \"content-type\" value: \"application/text\" key: type: STRING data: | \"bar\" value: type: JSON data: | \"eyJmb28iOiAiYmFyIn0K\" ","categories":"","description":"Learn how to manage a KTable Topic Records\n","excerpt":"Learn how to manage a KTable Topic Records\n","ref":"/jikkou/docs/user-guide/kafka/resources/ktable-records/","tags":["feature","resources"],"title":"Kafka Table Records"},{"body":" Here, you will find information to use the Schema Registry extensions.\nMore information:\n","categories":"","description":"Extensions for Schema Registry.\n","excerpt":"Extensions for Schema Registry.\n","ref":"/jikkou/docs/user-guide/schema-registry/","tags":"","title":"Schema Registry"},{"body":" SchemaRegistrySubject resources are used to define the subject schemas you want to manage on your Schema Registry. A SchemaRegistrySubject resource defines the schema, the compatibility level, and the references to be associated with a subject version.\nSchemaRegistrySubject Specification Here is the resource definition file for defining a SchemaRegistrySubject.\napiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"SchemaRegistrySubject\" # The resource kind (required) metadata: name: \u003cThe name of the subject\u003e # (required) labels: { } annotations: { } spec: schemaRegistry: vendor: 'Karapace' # (optional) The vendor of the Schema Registry compatibilityLevel: \u003ccompatibility_level\u003e # (optional) The schema compatibility level for this subject. schemaType: \u003cThe schema format\u003e # (required) Accepted values are: AVRO, PROTOBUF, JSON schema: $ref: \u003curl or path\u003e # references: # Specifies the names of referenced schemas (optional array). - name: \u003c\u003e # The name for the reference. subject: \u003c\u003e # The subject under which the referenced schema is registered. version: \u003c\u003e # The exact version of the schema under the registered subject. ] The metadata.name property is mandatory and specifies the name of the Subject.\nTo use the SchemaRegistry default values for the compatibilityLevel you can omit the property.\nExample Here is a simple example that shows how to define a single subject AVRO schema for type using the SchemaRegistrySubject resource type.\nfile: subject-user.yaml\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"SchemaRegistrySubject\" metadata: name: \"User\" labels: { } annotations: schemaregistry.jikkou.io/normalize-schema: true spec: compatibilityLevel: \"FULL_TRANSITIVE\" schemaType: \"AVRO\" schema: $ref: ./user-schema.avsc file: user-schema.avsc\n--- { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"name\", \"type\": [ \"null\", \"string\" ], \"default\": null, }, { \"name\": \"favorite_number\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"favorite_color\", \"type\": [ \"null\", \"string\" ], \"default\": null } ] } Alternatively, we can directly pass the Avro schema as follows :\nfile: subject-user.yaml\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"SchemaRegistrySubject\" metadata: name: \"User\" labels: { } annotations: schemaregistry.jikkou.io/normalize-schema: true spec: compatibilityLevel: \"FULL_TRANSITIVE\" schemaType: \"AVRO\" schema: | { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"name\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"favorite_number\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"favorite_color\", \"type\": [ \"null\", \"string\"], \"default\": null } ] } ","categories":"","description":"Learn how to manage Schema Registry Subject Schema in Aiven.\n","excerpt":"Learn how to manage Schema Registry Subject Schema in Aiven.\n","ref":"/jikkou/docs/user-guide/aiven/resources/schema-registry-subject/","tags":["feature","resources"],"title":"Subject for Aiven Schema Registry"},{"body":" Here, you will find information to use the Aiven for Kafka extensions.\nMore information:\n","categories":"","description":"Extensions for Aiven\n","excerpt":"Extensions for Aiven\n","ref":"/jikkou/docs/user-guide/aiven/","tags":"","title":"Aiven"},{"body":"Jikkou ships with the following built-in validations:\nNo validation\n","categories":"","description":"Learn how to use the built-in validations provided by the extensions for Aiven.\n","excerpt":"Learn how to use the built-in validations provided by the extensions ‚Ä¶","ref":"/jikkou/docs/user-guide/aiven/validations/","tags":["feature","extensions"],"title":"Validations"},{"body":"Jikkou ships with the following built-in validations:\n","categories":"","description":"Learn how to use the validations provided by the Kafka Connect extension.\n","excerpt":"Learn how to use the validations provided by the Kafka Connect ‚Ä¶","ref":"/jikkou/docs/user-guide/kafka-connect/validations/","tags":["feature","extensions"],"title":"Validations"},{"body":"Jikkou ships with the following built-in validations:\nTopics NoDuplicateTopicsAllowedValidation (auto registered)\nTopicConfigKeysValidation (auto registered)\ntype = io.streamthoughts.jikkou.kafka.validation.TopicConfigKeysValidation The TopicConfigKeysValidation allows checking if the specified topic configs are all valid.\nTopicMinNumPartitions type = io.streamthoughts.jikkou.kafka.validation.TopicMinNumPartitionsValidation The TopicMinNumPartitions allows checking if the specified number of partitions for a topic is not less than the minimum required.\nConfiguration\nName Type Description Default topicMinNumPartitions Int Minimum number of partitions allowed TopicMaxNumPartitions type = io.streamthoughts.jikkou.kafka.validation.TopicMaxNumPartitions The TopicMaxNumPartitions allows checking if the number of partitions for a topic is not greater than the maximum configured.\nConfiguration\nName Type Description Default topicMaxNumPartitions Int Maximum number of partitions allowed TopicMinReplicationFactor type = io.streamthoughts.jikkou.kafka.validation.TopicMinReplicationFactor The TopicMinReplicationFactor allows checking if the specified replication factor for a topic is not less than the minimum required.\nConfiguration\nName Type Description Default topicMinReplicationFactor Int Minimum replication factor allowed TopicMaxReplicationFactor type = io.streamthoughts.jikkou.kafka.validation.TopicMaxReplicationFactor The TopicMaxReplicationFactor allows checking if the specified replication factor for a topic is not greater than the maximum configured.\nConfiguration\nName Type Description Default topicMaxReplicationFactor Int Maximum replication factor allowed TopicNamePrefix type = io.streamthoughts.jikkou.kafka.validation.TopicNamePrefix The TopicNamePrefix allows checking if the specified name for a topic starts with one of the configured suffixes.\nConfiguration\nName Type Description Default topicNamePrefixes List List of topic name prefixes allows TopicNameSuffix type = io.streamthoughts.jikkou.kafka.validation.TopicNameSuffix The TopicNameSuffix allows checking if the specified name for a topic ends with one of the configured suffixes.\nConfiguration\nName Type Description Default topicNameSuffixes List List of topic name suffixes allows ACLs NoDuplicateUsersAllowedValidation (auto registered)\nNoDuplicateRolesAllowedValidation (auto registered)\nQuotas QuotasEntityValidation (auto registered)\n","categories":"","description":"Learn how to use the built-in validations provided by the extensions for Apache Kafka.\n","excerpt":"Learn how to use the built-in validations provided by the extensions ‚Ä¶","ref":"/jikkou/docs/user-guide/kafka/validations/","tags":["feature","extensions"],"title":"Validations"},{"body":"Jikkou ships with the following built-in validations:\nSubject SchemaCompatibilityValidation type = io.streamthoughts.jikkou.schema.registry.validation.SchemaCompatibilityValidation The SchemaCompatibilityValidation allows testing the compatibility of the schema with the latest version already registered in the Schema Registry using the provided compatibility-level.\nAvroSchemaValidation The AvroSchemaValidation allows checking if the specified Avro schema matches some specific avro schema definition rules;\ntype = io.streamthoughts.jikkou.schema.registry.validation.AvroSchemaValidation Configuration\nName Type Description Default fieldsMustHaveDoc Boolean Verify that all record fields have a doc property false fieldsMustBeNullable Boolean Verify that all record fields are nullable false fieldsMustBeOptional Boolean Verify that all record fields are optional false ","categories":"","description":"Learn how to use the built-in validations provided by the extensions for Schema Registry.\n","excerpt":"Learn how to use the built-in validations provided by the extensions ‚Ä¶","ref":"/jikkou/docs/user-guide/schema-registry/validations/","tags":["feature","extensions"],"title":"Validations"},{"body":" Here, you will find information about the annotations provided by the Aiven extension for Jikkou.\nList of built-in annotations kafka.aiven.io/acl-entry-id Used by jikkou.\nThe annotation is automatically added by Jikkou to describe the ID of an ACL entry.\n","categories":"","description":"Learn how to use the metadata annotations provided by the extensions for Aiven.\n","excerpt":"Learn how to use the metadata annotations provided by the extensions ‚Ä¶","ref":"/jikkou/docs/user-guide/aiven/annotations/","tags":"","title":"Annotations"},{"body":" This section lists a number of well known annotations, that have defined semantics. They can be attached to KafkaConnect resources through the metadata.annotations field and consumed as needed by extensions (i.e., validations, transformations, controller, collector, etc.).\nList of built-in annotations ","categories":"","description":"Learn how to use the metadata annotations provided by the Kafka Connect extension.\n","excerpt":"Learn how to use the metadata annotations provided by the Kafka ‚Ä¶","ref":"/jikkou/docs/user-guide/kafka-connect/annotations/","tags":"","title":"Annotations"},{"body":" Here, you will find information about the annotations provided the Apache Kafka extension for Jikkou.\nList of built-in annotations kafka.jikkou.io/cluster-id Used by jikkou.\nThe annotation is automatically added by Jikkou to a describe object part of an Apache Kafka cluster.\n","categories":"","description":"Learn how to use the metadata annotations provided by the extensions for Apache Kafka.\n","excerpt":"Learn how to use the metadata annotations provided by the extensions ‚Ä¶","ref":"/jikkou/docs/user-guide/kafka/annotations/","tags":"","title":"Annotations"},{"body":" Here, you will find information about the annotations provided by the Schema Registry extension for Jikkou.\nList of built-in annotations schemaregistry.jikkou.io/url Used by jikkou.\nThe annotation is automatically added by Jikkou to describe the SchemaRegistry URL from which a subject schema is retrieved.\nschemaregistry.jikkou.io/schema-version Used by jikkou.\nThe annotation is automatically added by Jikkou to describe the version of a subject schema.\nschemaregistry.jikkou.io/schema-id Used by jikkou.\nThe annotation is automatically added by Jikkou to describe the version of a subject id.\nschemaregistry.jikkou.io/normalize-schema Used on: schemaregistry.jikkou.io/v1beta2:SchemaRegistrySubject\nThis annotation can be used to normalize the schema on SchemaRegistry server side. Note, that Jikkou will attempt to normalize AVRO and JSON schema.\nSee: Confluent SchemaRegistry API Reference\nschemaregistry.jikkou.io/permanante-delete Used on: schemaregistry.jikkou.io/v1beta2:SchemaRegistrySubject\nThe annotation can be used to specify a hard delete of the subject, which removes all associated metadata including the schema ID. The default is false. If the flag is not included, a soft delete is performed. You must perform a soft delete first, then the hard delete.\nSee: Confluent SchemaRegistry API Reference\n","categories":"","description":"Learn how to use the metadata annotations provided by the extensions for Schema Registry.\n","excerpt":"Learn how to use the metadata annotations provided by the extensions ‚Ä¶","ref":"/jikkou/docs/user-guide/schema-registry/annotations/","tags":"","title":"Annotations"},{"body":" This section lists a number of well known labels, that have defined semantics. They can be attached to KafkaConnect resources through the metadata.labels field and consumed as needed by extensions (i.e., validations, transformations, controller, collector, etc.).\nLabels kafka.jikkou.io/connect-cluster # Example --- apiVersion: \"kafka.jikkou.io/v1beta1\" kind: \"KafkaConnector\" metadata: labels: kafka.jikkou.io/connect-cluster: 'my-connect-cluster' The value of this label defined the name of the Kafka Connect cluster to create the connector instance in. The cluster name must be configured through the kafkaConnect.clusters[] Jikkou‚Äôs configuration setting (see: Configuration).\n","categories":"","description":"Learn how to use the metadata labels provided by the Kafka Connect extension.\n","excerpt":"Learn how to use the metadata labels provided by the Kafka Connect ‚Ä¶","ref":"/jikkou/docs/user-guide/kafka-connect/labels/","tags":"","title":"Labels"},{"body":"The Apache Kafka extension for Jikkou utilizes the Kafka Admin Client which is compatible with any Kafka infrastructure, such as :\nAiven Apache Kafka Confluent Cloud MSK Redpanda etc. In addition, Kafka Protocol has a ‚Äúbidirectional‚Äù client compatibility policy. In other words, new clients can talk to old servers, and old clients can talk to new servers.\n","categories":"","description":"Compatibility for Apache Kafka.\n","excerpt":"Compatibility for Apache Kafka.\n","ref":"/jikkou/docs/user-guide/kafka/compatibility/","tags":"","title":"Compatibility"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/tags/concept/","tags":"","title":"concept"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/tags/docs/","tags":"","title":"docs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/tags/extension/","tags":"","title":"extension"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/tags/extensions/","tags":"","title":"extensions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/tags/feature/","tags":"","title":"feature"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/tags/how-to/","tags":"","title":"how-to"},{"body":" Jikkou (jikk≈ç / ÂÆüË°å)! Efficient and easy automation and provisioning of resources for any Kafka infrastructure. Learn More Download Jikkou (jikk≈ç / ÂÆüË°å) is a lightweight open-source tool designed to provide an efficient and easy way to manage, automate and provision resources on your self-service Event-Driven Data Mesh platforms (or, more simply, on any Apache Kafka Infrastructures).\nDeveloped by Kafka ‚ù§Ô∏è, Jikkou aims to streamline daily operations on Apache Kafka, ensuring that platform governance is no longer a boring and tedious task for both Developers and Administrators.\nJikkou enables a declarative management approach of Topics, ACLs, Quotas, Schemas, Connectors and even more with the use of YAML files called Resource Definitions.\nTaking inspiration from kubectl and Kubernetes resource definition files, Jikkou offers an intuitive and user-friendly approach to configuration management.\nJikkou can be used with Apache Kafka, Aiven, Amazon MSK, Confluent Cloud, Redpanda.\nJikkou is distributed under the Apache 2.0 license. Apache, Apache Kafka, Kafka, and associated open source project names are trademarks of the Apache Software Foundation.\nContributions welcome Want to join the fun on Github? New users are always welcome!\nContribute Support Jikkou Team Add a star to the GitHub project, it only takes 5 seconds!\nStar ","categories":"","description":"","excerpt":" Jikkou (jikk≈ç / ÂÆüË°å)! Efficient and easy automation and provisioning ‚Ä¶","ref":"/jikkou/","tags":"","title":"Jikkou"},{"body":" This transformation can be used to enforce a maximum value for the number of partitions of kafka topics.\nConfiguration Name Type Description Default maxNumPartitions Int maximum value for the number of partitions to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMaxNumPartitions priority = 100 config = { maxNumPartitions = 50 } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a maximum value for the ‚Ä¶","ref":"/jikkou/docs/user-guide/kafka/transformations/kafkatopicmaxnumpartitions/","tags":"","title":"KafkaTopicMaxNumPartitions"},{"body":" This transformation can be used to enforce a maximum value for the retention.ms property of kafka topics.\nConfiguration Name Type Description Default maxRetentionMs Int Minimum value of retention.ms to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMinRetentionMsTransformation priority = 100 config = { maxRetentionMs = 2592000000 # 30 days } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a maximum value for the ‚Ä¶","ref":"/jikkou/docs/user-guide/kafka/transformations/kafkatopicmaxretentionms/","tags":"","title":"KafkaTopicMaxRetentionMs"},{"body":" This transformation can be used to enforce a minimum value for the min.insync.replicas property of kafka topics.\nConfiguration Name Type Description Default minInSyncReplicas Int Minimum value of min.insync.replicas to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMinInSyncReplicasTransformation priority = 100 config = { minInSyncReplicas = 2 } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a minimum value for the ‚Ä¶","ref":"/jikkou/docs/user-guide/kafka/transformations/kafkatopicmininsyncreplicas/","tags":"","title":"KafkaTopicMinInSyncReplicas"},{"body":" This transformation can be used to enforce a minimum value for the replication factor of kafka topics.\nConfiguration Name Type Description Default minReplicationFactor Int Minimum value of replication factor to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMinReplicasTransformation priority = 100 config = { minReplicationFactor = 3 } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a minimum value for the ‚Ä¶","ref":"/jikkou/docs/user-guide/kafka/transformations/kafkatopicminreplicas/","tags":"","title":"KafkaTopicMinReplicas"},{"body":" This transformation can be used to enforce a minimum value for the retention.ms property of kafka topics.\nConfiguration Name Type Description Default minRetentionMs Int Minimum value of retention.ms to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMinRetentionMsTransformation priority = 100 config = { minRetentionMs = 604800000 # 7 days } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a minimum value for the ‚Ä¶","ref":"/jikkou/docs/user-guide/kafka/transformations/kafkatopicminretentionms/","tags":"","title":"KafkaTopicMinRetentionMs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/tags/resources/","tags":"","title":"resources"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/tags/","tags":"","title":"Tags"}]