[{"body":"You can use a ConfigMap to define reusable data in the form of key/value pairs that can then be referenced and used by other resources.\nSpecification --- apiVersion: \"core.jikkou.io/v1beta2\" kind: ConfigMap metadata: name: '\u003cCONFIG-MAP-NAME\u003e' # Name of the ConfigMap (required) data: # Map of key-value pairs (required) \u003cKEY_1\u003e: \"\u003cVALUE_1\u003e\" Example For example, the below ConfigMap show how to define default config properties namedcKafkaTopicConfig that can then reference and used to define multiple KafkaTopic. resources.\n--- apiVersion: \"core.jikkou.io/v1beta2\" kind: ConfigMap metadata: name: 'KafkaTopicConfig' data: cleanup.policy: 'delete' min.insync.replicas: 2 retention.ms: 86400000 # (1 day) ","categories":"","description":"Learn how to use ConfigMap objects.\n","excerpt":"Learn how to use ConfigMap objects.\n","ref":"/jikkou/docs/user-guide/core/resources/configmap/","tags":"","title":"ConfigMap"},{"body":" Here, you will find information to use the Core extensions.\nMore information:\n","categories":"","description":"Core extensions for Jikkou\n","excerpt":"Core extensions for Jikkou\n","ref":"/jikkou/docs/user-guide/core/","tags":"","title":"Core"},{"body":"This document will guide you through setting up Jikkou in a few minutes and managing your first resources with Jikkou.\nPrerequisites The following prerequisites are required for a successful and properly use of Jikkou.\nMake sure the following is installed:\nAn Apache Kafka cluster. Using Docker, Docker Compose is the easiest way to use it. Java 17 Start your local Apache Kafka Cluster You must have access to an Apache Kafka cluster for using Jikkou. Most of the time, the latest version of Jikkou is always built for working with the most recent version of Apache Kafka.\nMake sure the Docker is up and running.\nThen, run the following commands:\n$ git clone https://github.com/streamthoughts/jikkou $ cd jikkou $ ./up # use ./down for stopping the docker-compose stack Run Jikkou Download the latest distribution Run the following commands to install the latest version:\nwget https://github.com/streamthoughts/jikkou/releases/download/0.20.0/jikkou.deb \u0026\u0026 \\ sudo dpkg -i jikkou.deb \u0026\u0026 \\ source \u003c(jikkou generate-completion) \u0026\u0026 \\ jikkou --version For more details, or for other options, see the installation guide.\nConfigure Jikkou for your local Apache Kafka cluster Set configuration context for localhost\njikkou config set-context localhost --config=kafka.client.bootstrap.servers=localhost:9092 Show the complete configuration.\njikkou config view --name localhost Finally, let’s check if your cluster is accessible:\njikkou health get kafkabroker (output)\nIf OK, you should get an output similar to :\n--- name: \"kafka\" status: \"UP\" details: resource: \"urn:kafka:cluster:id:KRzY-7iRTHy4d1UVyNlcuw\" brokers: - id: \"1\" host: \"localhost\" port: 9092 Create your first topics First, create a resource YAML file describing the topics you want to create on your cluster:\nfile: kafka-topics.yaml\napiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaTopicList\" items: - metadata: name: 'my-first-topic' spec: partitions: 5 replicationFactor: 1 configs: cleanup.policy: 'compact' - metadata: name: 'my-second-topic' spec: partitions: 4 replicationFactor: 1 configs: cleanup.policy: 'delete' Then, run the following Jikkou command to trigger the topic creation on the cluster:\njikkou create -f ./kafka-topics.yaml (output)\nTASK [ADD] Add topic 'my-first-topic' (partitions=5, replicas=-1, configs=[cleanup.policy=compact]) - CHANGED { \"changed\" : true, \"end\" : 1683986528117, \"resource\" : { \"name\" : \"my-first-topic\", \"partitions\" : { \"after\" : 5 }, \"replicas\" : { \"after\" : -1 }, \"configs\" : { \"cleanup.policy\" : { \"after\" : \"compact\", \"operation\" : \"ADD\" } }, \"operation\" : \"ADD\" }, \"failed\" : false, \"status\" : \"CHANGED\" } TASK [ADD] Add topic 'my-second-topic' (partitions=4, replicas=-1, configs=[cleanup.policy=delete]) - CHANGED { \"changed\" : true, \"end\" : 1683986528117, \"resource\" : { \"name\" : \"my-second-topic\", \"partitions\" : { \"after\" : 4 }, \"replicas\" : { \"after\" : -1 }, \"configs\" : { \"cleanup.policy\" : { \"after\" : \"delete\", \"operation\" : \"ADD\" } }, \"operation\" : \"ADD\" }, \"failed\" : false, \"status\" : \"CHANGED\" } EXECUTION in 772ms ok : 0, created : 2, altered : 0, deleted : 0 failed : 0 Tips In the above command, we chose to use the create command to create the new topics. But we could just as easily use the update or apply command to get the same result depending on our needs. Finally, you can verify that topics are created on the cluster\njikkou get kafkatopics --describe-default-configs Tips We use the --describe-default-configs to export built-in default configuration for configs that have a default value. Update Kafka Topics Edit your kafka-topics.yaml to add a retention.ms: 86400000 property to the defined topics.\nThen, run the following command.\njikkou update -f ./kafka-topics.yaml Delete Kafka Topics To delete all topics defines in the topics.yaml, add an annotation jikkou.io/delete: true as follows:\napiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaTopicList\" metadata: annotations: # Annotation to specify that all resources must be deleted. jikkou.io/delete: true items: - metadata: name: 'my-first-topic' spec: partitions: 5 replicationFactor: 1 configs: cleanup.policy: 'compact' - metadata: name: 'my-second-topic' spec: partitions: 4 replicationFactor: 1 configs: cleanup.policy: 'delete' Then, run the following command:\n$ jikkou apply \\ --files ./kafka-topics.yaml \\ --selector \"metadata.name MATCHES (my-.*-topic)\" \\ --dry-run Using the dry-run option, give you the possibility to check the changes that will be made before applying them.\nNow, rerun the above command without the --dry-run option to definitively delete the topics.\nRecommendation When working in a production environment, we strongly recommend running commands with a --selector option to ensure that changes are only applied to a specific set of resources. Also, always run your command in --dry-run mode to verify the changes that will be executed by Jikkou before continuing. Reading the Help To learn more about the available Jikkou commands, use jikkou help or type a command followed by the -h flag:\n$ jikkou help get Next Steps Now, you’re ready to use Jikkou!🚀\nAs next steps, we suggest reading the following documentation in this order:\nLearn Jikkou concepts Read the Developer Guide to understand how to use the Jikkou API for Java Look at the examples ","categories":"","description":"This guide covers how you can quickly get started using Jikkou.\n","excerpt":"This guide covers how you can quickly get started using Jikkou.\n","ref":"/jikkou/docs/introduction/getting_started/","tags":"","title":"Getting Started"},{"body":" This is a placeholder page that shows you how to use this template site.\n","categories":"","description":"Here's where your user finds out if your project is for them.\n","excerpt":"Here's where your user finds out if your project is for them.\n","ref":"/jikkou/docs/introduction/","tags":"","title":"Introduction"},{"body":" Jikkou Resources are entities that represent the state of a concrete instance of a concept that are part of the state of your system, like a Topic on an Apache Kafka cluster.\nResource Objects All resources can be distinguished between persistent objects, which are used to describe the desired state of your system, and transient objects, which are only used to enrich or provide additional capabilities for the definition of persistent objects.\nA resource is an object with a type (called a Kind) and a concrete model that describe the associated data. All resource are scoped by an API Group and Version.\nResource Definition Resources are described in YAML format.\nHere is a sample resource that described a Kafka Topic.\napiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopic metadata: name: 'my-topic' labels: environment: test annotations: {} spec: partitions: 1 replicas: 1 configs: min.insync.replicas: 1 cleanup.policy: 'delete' Resource Properties The following are the properties that can be set to describe a resource:\nProperty Description apiVersion The group/version of the resource type. kind The type of the describe resource. metadata.name An optional name to identify the resource. metadata.labels Arbitrary metadata to attach to the resource that can be handy when you have a lot of resources and you only need to identity or filters some objects. metadata.annotations Arbitrary non-identifying metadata to attach to the resource to mark them for a specific operation or to record some metadata. spec The object properties describing a desired state ","categories":"","description":"","excerpt":" Jikkou Resources are entities that represent the state of a concrete …","ref":"/jikkou/docs/concepts/resource/","tags":["concept"],"title":"Resource"},{"body":" Here, you will find the list of core resources supported for Jikkou.\nCore Resources More information:\n","categories":"","description":"","excerpt":" Here, you will find the list of core resources supported for Jikkou. …","ref":"/jikkou/docs/user-guide/core/resources/","tags":"","title":"Resources"},{"body":" Here, you will find information to use the Apache Kafka extensions.\nMore information:\n","categories":"","description":"Extensions for Apache Kafka.\n","excerpt":"Extensions for Apache Kafka.\n","ref":"/jikkou/docs/user-guide/kafka/","tags":"","title":"Apache Kafka"},{"body":" This section explains key concepts used within Jikkou:\n","categories":"","description":"Learn the differents concepts used within Jikkou\n","excerpt":"Learn the differents concepts used within Jikkou\n","ref":"/jikkou/docs/concepts/","tags":"","title":"Concepts"},{"body":" Here, you will find the list of resources supported by the extension for Aiven.\nConfiguration You can configure the properties to be used to connect the Aiven service through the Jikkou client configuration property jikkou.aiven.\nExample:\njikkou { aiven { # Aiven project name project = \"http://localhost:8081\" # Aiven service name service = generic # URL to the Aiven REST API. apiUrl = \"https://api.aiven.io/v1/\" # Aiven Bearer Token. Tokens can be obtained from your Aiven profile page tokenAuth = null # Enable debug logging debugLoggingEnabled = false } } ","categories":"","description":"Learn how to configure the extensions for Aiven.\n","excerpt":"Learn how to configure the extensions for Aiven.\n","ref":"/jikkou/docs/user-guide/aiven/configuration/","tags":"","title":"Configuration"},{"body":" Here, you will find the list of resources supported for Apache Kafka.\nConfiguration The Apache Kafka extension is built on top of the Kafka Admin Client. You can configure the properties to be passed to kafka client through the Jikkou client configuration property jikkou.kafka.client.\nExample:\njikkou { kafka { client { bootstrap.servers = \"localhost:9092\" security.protocol = \"SSL\" ssl.keystore.location = \"/tmp/client.keystore.p12\" ssl.keystore.password = \"password\" ssl.keystore.type = \"PKCS12\" ssl.truststore.location = \"/tmp/client.truststore.jks\" ssl.truststore.password = \"password\" ssl.key.password = \"password\" } } } In addition, the extension support configuration settings to wait for at least a minimal number of brokers before processing.\njikkou { kafka { brokers { # If 'True' waitForEnabled = true waitForEnabled = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_ENABLED} # The minimal number of brokers that should be alive for the CLI stops waiting. waitForMinAvailable = 1 waitForMinAvailable = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_MIN_AVAILABLE} # The amount of time to wait before verifying that brokers are available. waitForRetryBackoffMs = 1000 waitForRetryBackoffMs = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_RETRY_BACKOFF_MS} # Wait until brokers are available or this timeout is reached. waitForTimeoutMs = 60000 waitForTimeoutMs = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_TIMEOUT_MS} } } } ","categories":"","description":"Learn how to configure the extensions for Apache Kafka.\n","excerpt":"Learn how to configure the extensions for Apache Kafka.\n","ref":"/jikkou/docs/user-guide/kafka/configuration/","tags":"","title":"Configuration"},{"body":" Here, you will find the list of resources supported for SchemaRegistry.\nConfiguration You can configure the properties to be used to connect the SchemaRegistry service through the Jikkou client configuration property jikkou.schemaRegistry.\nExample:\njikkou { schemaRegistry { # Comma-separated list of URLs for schema registry instances that can be used to register or look up schemas url = \"http://localhost:8081\" # The name of the schema registry implementation vendor - can be any value vendor = generic # Method to use for authenticating on Schema Registry. Available values are: [none, basicauth] authMethod = none # Use when 'schemaRegistry.authMethod' is 'basicauth' to specify the username for Authorization Basic header basicAuthUser = null # Use when 'schemaRegistry.authMethod' is 'basicauth' to specify the password for Authorization Basic header basicAuthPassword = null # Enable debug logging debugLoggingEnabled = false } } ","categories":"","description":"Learn how to configure the extensions for SchemaRegistry.\n","excerpt":"Learn how to configure the extensions for SchemaRegistry.\n","ref":"/jikkou/docs/user-guide/schema-registry/configuration/","tags":"","title":"Configuration"},{"body":" Jikkou can be installed either from source, or from releases.\nFrom The Jikkou Project Releases Every release released versions of Jikkou is available:\nAs a zip/tar.gz package from GitHub Releases As a fatJar available from Maven Central As a docker image available from Docker Hub. As a Debian package from GitHub Releases These are the official ways to get Jikkou releases that you manually downloaded and installed.\nInstall From Tarball distribution Download your desired version Unpack it (tar -zxvf jikkou-0.20.0-runner.tar.gz) Move the unpacked directory to its desired destination (mv jikkou-0.20.0-runner /opt) Add the executable to your PATH (export PATH=$PATH:/opt/jikkou/bin) Install From Debian distribution Download your desired version $ wget https://github.com/streamthoughts/jikkou/releases/download/0.20.0/jikkou.deb $ sudo dpkg -i jikkou.deb or just run the command:\ncurl -s https://raw.githubusercontent.com/streamthoughts/jikkou/main/get.sh | sh From there, you should be able to run the client: jikkou help.\nNote Jikkou will install itself in the directory : /opt/jikkou It is recommended to install the bash/zsh completion script jikkou_completion:\nwget https://raw.githubusercontent.com/streamthoughts/jikkou/master/jikkou_completion . jikkou_completion or alternatively, run the following command for generation the completion script.\n$ source \u003c(jikkou generate-completion) Development Builds In addition to releases you can download or install development snapshots of Jikkou.\nFrom Docker Hub Docker images are built and push to Docker Hub from the latest main branch. They are not official releases, and may not be stable. However, they offer the opportunity to test the cutting edge features.\n$ docker run -it streamthoughts/jikkou:master From Source (Linux, macOS) Building Jikkou from source is slightly more work, but is the best way to go if you want to test the latest (pre-release) Jikkou version.\nPrerequisites You must have a working Java environment with a JDK 17 (i.e. $JAVA_HOME environment variable is configured).\nBuild Tarball / Debian packages $ git clone https://github.com/streamthoughts/jikkou.git $ cd jikkou $ ./mvnw clean package -DskipTests -Pdist Then, distributions will be available in the ./dist directory:\n. ├── jikkou-\u003cVERSION\u003e-runner │ └── jikkou-\u003cVERSION\u003e-runner │ ├── bin │ │ ├── jikkou │ │ └── jikkou.bat │ ├── etc │ │ └── logback.xml │ ├── jikkou_completion │ ├── lib │ │ └── jikkou-runner.jar │ ├── LICENSE │ └── README.adoc ├── jikkou-\u003cVERSION\u003e-runner.tar.gz ├── jikkou-\u003cVERSION\u003e-runner.zip └── jikkou.deb Build RPM In addition to that, you may need to install Jikkou from an RPM package. For doing that, you can run the following commands:\n$ ./mvnw clean package -DskipTests -Prpm The RPM package will available in the ./target/rpm/jikkou/RPMS/noarch/ directory.\n","categories":"","description":"This guide shows how to install the Jikkou CLI.\n","excerpt":"This guide shows how to install the Jikkou CLI.\n","ref":"/jikkou/docs/introduction/installation/","tags":"","title":"Installing Jikkou"},{"body":" Jikkou is not only a CLI but also a Java library that you can use internally in your project.\nExamples Create Kafka Topics The example below shows how to use the JikkouApi to create Kafka Topics from resource description file.\nYou will need to add the following dependency in the pom.xml file of your project.\n\u003cdependency\u003e \u003cgroupId\u003eio.streamthoughts\u003c/groupId\u003e \u003cartifactId\u003ejikkou-extension-kafka\u003c/artifactId\u003e \u003cversion\u003e${jikkou.version}\u003c/version\u003e \u003c/dependency\u003e public class CreateKafkaTopicsExample { public static void main(String[] args) { // (1) Register Kinds ResourceDeserializer.registerKind(V1KafkaTopic.class); ResourceDeserializer.registerKind(V1KafkaTopicList.class); // (2) Load Resources HasItems resources = ResourceLoader.create().load(List.of(\"./kafka-topics.yaml\")); // (3) Create and configure Jikkou API AdminClientContext clientContext = new AdminClientContext( () -\u003e AdminClient.create(Map.of(\"bootstrap.servers\", \"localhost:9092\")) ); try (JikkouApi api = DefaultApi.builder() .withCollector(new AdminClientKafkaTopicCollector(clientContext)) .withController(new AdminClientKafkaTopicController(clientContext)) .build()) { // (4) Execute Reconciliation List\u003cChangeResult\u003cChange\u003e\u003e changes = api.apply( resources, ReconciliationMode.CREATE, ReconciliationContext.with(false) // dry-run ); // (5) Do something with changes System.out.println(changes); } } } ","categories":"","description":"Learn how to use the Jikkou programmatically\n","excerpt":"Learn how to use the Jikkou programmatically\n","ref":"/jikkou/docs/developer-guide/api/","tags":"","title":"JikkouApi"},{"body":"Labels You can use labels to attach arbitrary identifying metadata to objects.\nLabels are key/value maps:\nmetadata: labels: \"key1\": \"value-1\" \"key2\": \"value-2\" Note The keys in the map must be string, but values can be any scalar types (string, boolean, or numeric). Labels are not persistent Jikkou is completely stateless. In other words, it will not store any state about the describe resources objects. Thus, when retrieving objects from your system labels may not be reattached to the metadata objects. Example metadata: labels: environment: \"stating\" Annotations You can use annotations to attach arbitrary non-identifying metadata to objects.\nAnnotations are key/value maps:\nmetadata: annotations: key1: \"value-1\" key2: \"value-2\" Note The keys in the map must be string, but the values can be of any scalar types (string, boolean, or numeric). Built-in Annotations jikkou.io/ignore Used on: All Objects.\nThis annotation indicates whether the object should be ignored for reconciliation.\njikkou.io/bypass-validations Used on: All Objects.\nThis annotation indicates whether the object should bypass the validation chain. In other words, no validations will be applied on the object.\njikkou.io/delete Used on: All Objects.\nThis annotation indicates (when set to true) that the object should be deleted from your system.\njikkou.io/resource-location Used by jikkou.\nThis annotation is automatically added by Jikkou to an object when loaded from your local filesystem.\njikkou.io/items-count Used by jikkou.\nThis annotation is automatically added by Jikkou to an object collection grouping several resources of homogeneous type.\n","categories":"","description":"","excerpt":"Labels You can use labels to attach arbitrary identifying metadata to …","ref":"/jikkou/docs/concepts/labels-and-annotations/","tags":"","title":"Labels and annotations"},{"body":" Here, you will find the list of resources supported by the extensions for Aiven.\nAiven for Apache Kafka Resources More information:\n","categories":"","description":"Learn how to use the built-in resources provided by the extensions for Aiven.\n","excerpt":"Learn how to use the built-in resources provided by the extensions for …","ref":"/jikkou/docs/user-guide/aiven/resources/","tags":"","title":"Resources"},{"body":" Here, you will find the list of resources supported for Apache Kafka.\nApache Kafka Resources More information:\n","categories":"","description":"Learn how to use the built-in resources provided by the extensions for Apache Kafka.\n","excerpt":"Learn how to use the built-in resources provided by the extensions for …","ref":"/jikkou/docs/user-guide/kafka/resources/","tags":"","title":"Resources"},{"body":" Here, you will find the list of resources supported for Schema Registry.\nSchema Registry Resources More information:\n","categories":"","description":"Learn how to use the built-in resources provided by the extensions for Schema Registry.\n","excerpt":"Learn how to use the built-in resources provided by the extensions for …","ref":"/jikkou/docs/user-guide/schema-registry/resources/","tags":"","title":"Resources"},{"body":" Here, you will find information to use the built-in transformations for Apache Kafka resources.\nMore information:\n","categories":"","description":"Learn how to use the built-in transformation provided by the extensions for Apache Kafka.\n","excerpt":"Learn how to use the built-in transformation provided by the …","ref":"/jikkou/docs/user-guide/kafka/transformations/","tags":"","title":"Transformations"},{"body":" In the context of Jikkou, reconciliation refers to the process of comparing the desired state of an object with the actual state of the system and making any necessary corrections or adjustments to align them.\nChanges A Change represents a difference, detected during reconciliation, between two objects that can reconciled or corrected by adding, updating, or deleting an object or property attached to the actual state of the system.\nA Change represents a detected difference between two objects during the reconciliation process. These differences can be reconciled or corrected by adding, updating, or deleting an object or property associated with the actual state of the system\nJikkou identifies four types of changes:\nADD: Indicates the addition of a new object or property to an existing object.\nUPDATE: Indicates modifications made to an existing object or property of an existing object.\nDELETE: Indicates the removal of an existing object or property of an existing object.\nNONE: Indicates that no changes were made to an existing object or property.\nReconciliation Modes Depending on the chosen reconciliation mode, only specific types of changes will be applied.\nJikkou provides four distinct reconciliation modes that determine the types of changes to be applied:\nCREATE: This mode only applies changes that create new resource objects in your system. DELETE: This mode only applies changes that delete existing resource objects in your system. UPDATE: This mode only applies changes that create or update existing resource objects in your system. APPLY_ALL: This mode applies all changes to ensure that the actual state of a resource in the cluster matches the desired state defined in your resource definition file, regardless of the specific type of change. Each mode corresponds to a command offered by the Jikkou CLI (i.e., create, update, delete, and apply). Choose the appropriate mode based on your requirements.\nUsing JIKKOU CLI Some reconciliation modes might not be supported for all resources. Use jikkou extensions list --type Controller to check which actions could be perfomed for each resources. Reconciliation Options Depending on the type of resources being reconciled, the controller that will be involved in the reconciliation process might accept some options (i.e., using --options argument).\nMark Resource for Deletion To delete all the states associated with resource’s entities, you must add the following annotation to the resource definition:\nmetadata: annotations: jikkou.io/delete: true ","categories":"","description":"","excerpt":" In the context of Jikkou, reconciliation refers to the process of …","ref":"/jikkou/docs/concepts/reconciliation/","tags":["concept"],"title":"Reconciliation"},{"body":" Here, you will find information to use the Schema Registry extensions.\nMore information:\n","categories":"","description":"Extensions for Schema Registry.\n","excerpt":"Extensions for Schema Registry.\n","ref":"/jikkou/docs/user-guide/schema-registry/","tags":"","title":"Schema Registry"},{"body":" The User Guide section helps you learn more about the functionalities of Jikkou.\n","categories":"","description":"Learn how to use Jikkou for managing resource entities on you Apache Kafka cluster.\n","excerpt":"Learn how to use Jikkou for managing resource entities on you Apache …","ref":"/jikkou/docs/user-guide/","tags":["how-to","docs"],"title":"User Guide"},{"body":" This guide explains the basics of using Jikkou to manage resource entities on your Apache Kafka cluster. It assumes that you have already installed the Jikkou client. If you are simply interested in running a few quick commands, you may wish to begin with the Quickstart Guide. This chapter covers the particulars of Jikkou commands, and explains how to configure and use Jikkou.\nJikkou Architecture Components Jikkou is a tool which is implemented into two distinct parts:\nThe Jikkou Client is a command-line client for end users. The Jikkou Java Library provides the logic for executing all operations the Apache Kafka cluster. The Jikkou library is available on Maven Central\nFor Maven:\n\u003cdependency\u003e \u003cgroupId\u003eio.streamthoughts\u003c/groupId\u003e \u003cartifactId\u003ejikkou-api\u003c/artifactId\u003e \u003cversion\u003e${jikkou.version}\u003c/version\u003e \u003c/dependency\u003e Implementation The Jikkou client and library are written in the Java programming language.\nIt is built on top of the Kafka’s Java AdminClient. Thus, it works out-of-the-box with most the Apache Kafka distributions and cloud provider managed services (e.g., Aiven, Confluent Cloud, etc).\nThe tool is completely stateless and thus does not store any state. Basically: Your kafka cluster is the state of Jikkou.\nConfiguration files are written in YAML.\nCLI Usage $ jikkou help Usage: jikkou [-hV] [COMMAND] Jikkou streamlines the management of the configurations that live on your data streams platform. Find more information at: https://streamthoughts.github.io/jikkou/. Options: -h, --help Show this help message and exit. -V, --version Print version information and exit. Commands: create Create resources from the resource definition files (only non-existing resources will be created). delete Delete resources that are no longer described by the resource definition files. update Create or update resources from the resource definition files apply Update the resources as described by the resource definition files. resources List supported resources extensions List or describe the extensions of Jikkou config Sets or retrieves the configuration of this client diff Display all resource changes. validate Validate resource definition files. health Print or describe health indicators. help Display help information about the specified command. get List and describe all resources of a specific kind. Configuration To set up the configuration settings used by Jikkou CLI, you will need create a jikkou config file, which is created automatically when you create a configuration context using:\njikkou config set-context \u003ccontext-name\u003e [--config-file=\u003cconfig-gile\u003e] [--config=\u003cconfig-value\u003e] By default, the configuration of jikkou is located under the path ~/.jikkou/config.\nThis jikkou config file defines all the contexts that can be used by jikkou CLI.\nFor example, below is the config file created during the Getting Started.\n{ \"currentContext\" : \"localhost\", \"localhost\" : { \"configFile\": null, \"configProps\" : { \"kafka.client.bootstrap.servers\" : \"localhost:9092\" } } } Most of the time, a context does not directly contain the configuration properties to be used, but rather points to a specific HOCON (Human-Optimized Config Object Notation) through the (configFile).\nThen, the configProps allows you to override some of the property define by this file.\nIn addition, if no configuration file path is specified, Jikkou will lookup for an application.conf to those following locations:\n./application.conf $HOME/.jikkou/application.conf Finally, Jikkou always fallback to a reference configuration (see reference.conf).\njikkou { # The paths from which to load extensions extension.paths = [${?JIKKOU_EXTENSION_PATH}] # Kafka Extension kafka { # The default Kafka AdminClient configuration client { bootstrap.servers = \"localhost:9092\" bootstrap.servers = ${?JIKKOU_DEFAULT_KAFKA_BOOTSTRAP_SERVERS} } brokers { # If 'True' waitForEnabled = true waitForEnabled = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_ENABLED} # The minimal number of brokers that should be alive for the CLI stops waiting. waitForMinAvailable = 1 waitForMinAvailable = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_MIN_AVAILABLE} # The amount of time to wait before verifying that brokers are available. waitForRetryBackoffMs = 1000 waitForRetryBackoffMs = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_RETRY_BACKOFF_MS} # Wait until brokers are available or this timeout is reached. waitForTimeoutMs = 60000 waitForTimeoutMs = ${?JIKKOU_KAFKA_BROKERS_WAIT_FOR_TIMEOUT_MS} } } # The default custom transformations to apply on any resources. transformations = [] # The default custom validations to apply on any resources. validations = [ { name = \"topicMustHaveValidName\" type = io.streamthoughts.jikkou.kafka.validation.TopicNameRegexValidation priority = 100 config = { topicNameRegex = \"[a-zA-Z0-9\\\\._\\\\-]+\" topicNameRegex = ${?VALIDATION_DEFAULT_TOPIC_NAME_REGEX} } }, { name = \"topicMustHaveParitionsEqualsOrGreaterThanOne\" type = io.streamthoughts.jikkou.kafka.validation.TopicMinNumPartitionsValidation priority = 100 config = { topicMinNumPartitions = 1 topicMinNumPartitions = ${?VALIDATION_DEFAULT_TOPIC_MIN_NUM_PARTITIONS} } }, { name = \"topicMustHaveReplicasEqualsOrGreaterThanOne\" type = io.streamthoughts.jikkou.kafka.validation.TopicMinReplicationFactorValidation priority = 100 config = { topicMinReplicationFactor = 1 topicMinReplicationFactor = ${?VALIDATION_DEFAULT_TOPIC_MIN_REPLICATION_FACTOR} } } ] } Verify current configuration You can use jikkou config view to show the configuration currently used by your client.\nTips To debug the configuration use by Jikkou, you can run the following command: jikkou config view --comments or jikkou config view --debug ","categories":"","description":"This guide shows the basics of using Jikkou to manage resource entities on your Apache Kafka cluster.\n","excerpt":"This guide shows the basics of using Jikkou to manage resource …","ref":"/jikkou/docs/introduction/using_jikkou/","tags":"","title":"Using Jikkou"},{"body":" Here, you will find information to use the Aiven for Kafka extensions.\nMore information:\n","categories":"","description":"Extensions for Aiven\n","excerpt":"Extensions for Aiven\n","ref":"/jikkou/docs/user-guide/aiven/","tags":"","title":"Aiven"},{"body":" Here, you will find the necessary information to develop with the Jikkou API.\nMore information:\n","categories":"","description":"Learn how to use the Jikkou API\n","excerpt":"Learn how to use the Jikkou API\n","ref":"/jikkou/docs/developer-guide/","tags":"","title":"Developer Guide"},{"body":" You can use selectors to select only a subset of resource objects to describe from your system or for which you want to perform a reconciliation process.\nField Selector (default) Jikkou provides the built-in FieldSelector that allows you to filter resource objects based on a field key.\nSelector Expression The expression below shows you how to select only resource having a label environement equals to either staging or production.\nmetadata.labels.environement IN (staging,production) Expression Operators Five kinds of operators are supported:\nIN NOTIN EXISTS MATCHES DOESNOTMATCH Using JIKKOU CLI Selectors can be specified via the Jikkou CLI option: --selector. ","categories":"","description":"","excerpt":" You can use selectors to select only a subset of resource objects to …","ref":"/jikkou/docs/concepts/selectors/","tags":["concept","feature"],"title":"Selectors"},{"body":" ","categories":"","description":"What does your user need to know to try your project?\n","excerpt":"What does your user need to know to try your project?\n","ref":"/jikkou/docs/community/","tags":"","title":"Community"},{"body":" Transformations are applied to inbound resources. Transformations are used to transform, enrich, or filter resource entities before they are validated and thus before the reconciliation process is executed on them.\nAvailable Transformations You can list all the available transformations using the Jikkou CLI command:\njikkou extensions list --type=Transformation [-kinds \u003ca resource kind to filter returned results\u003e] Transformation chain When using Jikkou CLI, you can configure a transformation chain that will be applied to every resource. This chain consists of multiple transformations, each designed to handle different types of resources. Jikkou ensures that a transformation is executed only for the resource types it supports. In cases where a resource is not accepted by a transformation, it is passed to the next transformation in the chain. This process continues until a suitable transformation is found or until all transformations have been attempted.\nConfiguration jikkou { # The list of transformations to execute transformations: [ { # Simple or fully qualified class name of the transformation extension. type = \"\" # Priority to be used for executing this transformation extension. # The lowest value has the highest priority, so it's run first. Minimum value is -2^31 (highest) and a maximum value is 2^31-1 (lowest). # Usually, values under 0 should be reserved for internal transformation extensions. priority = 0 config = { # Configuration properties for this transformation } } ] } Tips The config object of a Transformation always fallback on the top-level jikkou config. This allows you to globally declare some properties of the validation configuration. Example jikkou { # The list of transformations to execute transformations: [ { # Enforce a minimum number of replicas for a kafka topic type = KafkaTopicMinReplicasTransformation priority = 100 config = { minReplicationFactor = 4 } }, { # Enforce a {@code min.insync.replicas} for a kafka topic. type = KafkaTopicMinInSyncReplicasTransformation priority = 100 config = { minInSyncReplicas = 2 } } ] } ","categories":"","description":"","excerpt":" Transformations are applied to inbound resources. Transformations are …","ref":"/jikkou/docs/concepts/transformations/","tags":["concept","feature","extension"],"title":"Transformations"},{"body":" This section regroups all frequently asked questions about Jikkou.\n","categories":"","description":"","excerpt":" This section regroups all frequently asked questions about Jikkou.\n","ref":"/jikkou/docs/frequently-asked-questions/","tags":"","title":"Frequently Asked Questions"},{"body":" Validations are applied to inbound resources to ensure that the resource entities adhere to specific rules or constraints. These validations are carried out after the execution of the transformation chain and before the reconciliation process takes place.\nAvailable Validations You can list all the available validations using the Jikkou CLI command:\njikkou extensions list --type=Validation [-kinds \u003ca resource kind to filter returned results\u003e] Validation chain When using Jikkou CLI, you can configure a validation chain that will be applied to every resource. This chain consists of multiple validations, each designed to handle different types of resources. Jikkou ensures that a validation is executed only for the resource types it supports. In cases where a resource is not accepted by a validation, it is passed to the next validation in the chain. This process continues until a suitable validation is found or until all validations have been attempted.\nConfiguration jikkou { # The list of transformations to execute validations: [ { # Custom name for the validation rule name = \"\" # Simple or fully qualified class name of the transformation extension. type = \"\" config = { # Configuration properties for this transformation } } ] } Tips The config object of a Validation always fallback on the top-level jikkou config. This allows you to globally declare some properties of the validation configuration. Example jikkou { # The list of transformations to execute validations: [ { # Custom name for the validation rule name = topicMustBePrefixedWithRegion # Simple or fully qualified class name of the validation extension. type = TopicNameRegexValidation # The config values that will be passed to the validation. config = { topicNameRegex = \"(europe|northamerica|asiapacific)-.+\" } } ] } ","categories":"","description":"","excerpt":" Validations are applied to inbound resources to ensure that the …","ref":"/jikkou/docs/concepts/validations/","tags":["concept","feature","extension"],"title":"Validations"},{"body":" Template helps you to dynamically define resource definition files from external data.\nTemplate Engine Jikkou provides a simple templating mechanism based-on Jinjava, a Jinja template engine for Java.\nRead the official documentation of Jinja to learn more about the syntax and semantics of the template engine.\nHow Does It Work ? Jikkou performs the rendering of your template in two phases:\nFirst, an initial rendering is performed using only the values and labels passed through the command-lines arguments. Thus, it is perfectly OK if your resource file is not initially a valid YAML file. Then, a second and final rendering is performed after parsing the YAML resource file using the additional values and labels as defined into the YAML resource file. Therefore, it’s important that your resource file is converted into a valid YAML file after the first rendering. Important You should use {% raw %}...{% endraw %} tags to ensure the variables defined into the template are not be interpreted during the first rendering. Variables Jikkou defines a number of top-level variables that are passed to the template engine.\nvalues:\nThe values passed into the template through the command-line --values-files and/or --set-value arguments In addition, values can be defined into the application.conf file and directly into the template file using the property template.values. By default, values is empty. labels:\nThe labels passed into the template through the command-line argument: --set-label. In addition, labels can be defined into the template file using the property metadata.labels. By default, labels is empty. system.env:\nThis provides access to all environment variables. system.props:\nThis provides access to all system properties. Template Values When using templating, a resource definition file may contain the additional property template. fields:\napiVersion: The api version (required) kind: The resource kind (required) metadata: labels: The set of key/value pairs that you can use to describe your resource file (optional) annotations: The set of key/value pairs automatically generated by the tool (optional) template: values: The set of key/value pairs to be passed to the template engine (optional) spec: Specification of the resource Values Data File Values Data File are used to define all the necessary values (i.e., the variables) to be used for generating a template.\nExample # file: ./values.yaml topicConfigs: partitions: 4 replicas: 3 topicPrefix: \"{{ system.env.TOPIC_PREFIX | default('test', true) }}\" countryCodes: - fr - be - de - es - uk - us Template Resource File Example # file: ./kafka-topics.tpl apiVersion: 'kafka.jikkou.io/v1beta2' kind: 'KafkaTopicList' items: { % for country in values.countryCodes % } - metadata: name: \"{{ values.topicPrefix}}-iot-events-{{ country }}\" spec: partitions: { { values.topicConfigs.partitions } } replicas: { { values.topicConfigs.replicas } } configMapRefs: - TopicConfig { % endfor % } --- apiVersion: \"core.jikkou.io/v1beta2\" kind: \"ConfigMap\" metadata: name: TopicConfig template: values: default_min_insync_replicas: \"{{ values.topicConfigs.replicas | default(3, true) | int | add(-1) }}\" data: retention.ms: 3600000 max.message.bytes: 20971520 min.insync.replicas: '{% raw %}{{ values.default_min_insync_replicas }}{% endraw %}' Command\n$ TOPIC_PREFIX=local jikkou validate --files topics.tpl --values-files values.yaml (Output)\n--- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaTopicList\" metadata: labels: { } annotations: jikkou.io/resource-location: \"file:///tmp/jikkou/topics.tpl\" spec: topics: - metadata: name: \"local-iot-events-fr\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" retention.ms: 3600000 max.message.bytes: 20971520 - metadata: name: \"local-iot-events-be\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" retention.ms: 3600000 max.message.bytes: 20971520 - metadata: name: \"local-iot-events-de\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" max.message.bytes: 20971520 retention.ms: 3600000 - metadata: name: \"local-iot-events-es\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" max.message.bytes: 20971520 retention.ms: 3600000 - metadata: name: \"local-iot-events-uk\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" max.message.bytes: 20971520 retention.ms: 3600000 - metadata: name: \"local-iot-events-us\" spec: partitions: 4 replicas: 3 configs: min.insync.replicas: \"2\" max.message.bytes: 20971520 retention.ms: 3600000 ","categories":"","description":"","excerpt":" Template helps you to dynamically define resource definition files …","ref":"/jikkou/docs/concepts/template/","tags":["concept","feature"],"title":"Template"},{"body":" Collectors are used to collect and describe all entities that exist into your system for a specific resource type.\nAvailable Collectors You can list all the available collectors using the Jikkou CLI command:\njikkou extensions list --type=Collector [-kinds \u003ca resource kind to filter returned results\u003e] ","categories":"","description":"","excerpt":" Collectors are used to collect and describe all entities that exist …","ref":"/jikkou/docs/concepts/collector/","tags":["concept","feature","extension"],"title":"Collectors"},{"body":" Controllers are used to compute and apply changes required to reconcile resources into a managed system.\nAvailable Controllers You can list all the available controllers using the Jikkou CLI command:\njikkou extensions list --type=Controller [-kinds \u003ca resource kind to filter returned results\u003e] ","categories":"","description":"","excerpt":" Controllers are used to compute and apply changes required to …","ref":"/jikkou/docs/concepts/controller/","tags":["concept","feature","extension"],"title":"Controllers"},{"body":" The KafkaTopicAclEntry resources are used to manage the Access Control Lists for Aiven Apache Kafka® service. A KafkaTopicAclEntry resource defines the permission to be granted to a user for one or more kafka topics.\nKafkaTopicAclEntry Specification Here is the resource definition file for defining a KafkaTopicAclEntry.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"KafkaTopicAclEntry\" # The resource kind (required) metadata: labels: { } annotations: { } spec: permission: \u003c\u003e # The permission. Accepted values are: READ, WRITE, ADMIN username: \u003c\u003e # The username topic: \u003c\u003e # Topic name or glob pattern Example Here is a simple example that shows how to define a single ACL entry using the KafkaTopicAclEntry resource type.\nfile: kafka-topic-acl-entry.yaml\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"KafkaTopicAclEntry\" metadata: labels: { } annotations: { } spec: permission: \"WRITE\" username: \"alice\" topic: \"public-*\" KafkaTopicAclEntryList If you need to define multiple ACL entries (e.g. using a template), it may be easier to use a KafkaTopicAclEntryList resource.\nSpecification Here the resource definition file for defining a KafkaTopicList.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"KafkaTopicAclEntryList\" # The resource kind (required) metadata: # (optional) name: \u003cThe name of the topic\u003e labels: { } annotations: { } items: [ ] # An array of KafkaTopicAclEntry Example Here is a simple example that shows how to define a single YAML file containing two ACL entry definitions using the KafkaTopicAclEntryList resource type.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"KafkaTopicAclEntryList\" items: - spec: permission: \"WRITE\" username: \"alice\" topic: \"public-*\" - spec: permission: \"READ\" username: \"bob\" topic: \"public-*\" ","categories":"","description":"Learn how to manage Access Control Lists for Aiven Apache Kafka®\n","excerpt":"Learn how to manage Access Control Lists for Aiven Apache Kafka®\n","ref":"/jikkou/docs/user-guide/aiven/resources/kafka-topic-acl/","tags":["feature","resources"],"title":"ACL for Aiven Apache Kafka®"},{"body":"Prerequisites Jdk 17 (see https://sdkman.io/ for installing java locally) Git Docker and Docker-Compose Your favorite IDE Building Jikkou We use Maven Wrapper to build our project. The simplest way to get started is:\nFor building distribution files.\n$ ./mvnw clean package -Pdist -DskipTests Alternatively, we also use Make to package and build the Docker image for Jikkou:\n$ make Running tests For running all tests and checks:\n$ ./mvnw clean verify Code Format This project uses the Maven plugin Spotless to format all Java classes and to apply some code quality checks.\nBugs \u0026 Security This project uses the Maven plugin SpotBugs and FindSecBugs to run some static analysis to look for bugs in Java code.\nReported bugs can be analysed using SpotBugs GUI:\n$ ./mvnw spotbugs:gui ","categories":"","description":"How to set up your environment for developing on Jikkou.\n","excerpt":"How to set up your environment for developing on Jikkou.\n","ref":"/jikkou/docs/community/developer-guide/","tags":"","title":"Developer Guide"},{"body":"Extensions Jikkou allows implementing and configuring extensions, i.e., Validation, and Transformer.\nJikkou’s sources are available onMaven Central\nFor Maven:\n\u003cdependency\u003e \u003cgroupId\u003eio.streamthoughts\u003c/groupId\u003e \u003cartifactId\u003ejikkou-api\u003c/artifactId\u003e \u003cversion\u003e${jikkou.version}\u003c/version\u003e \u003c/dependency\u003e For Gradle:\nimplementation group: 'io.streamthoughts', name: 'jikkou', version: ${jikkou.version} Packaging To make your extensions available to Jikkou, install them into one or many local directories. Then, use the jikkou.extension.paths property to configure the list of locations from which the extensions will be loaded.\nEach configured directories may contain:\nan uber JAR containing all the classes and third-party dependencies for the extensions. a directory containing all JARs for the extensions. ","categories":"","description":"","excerpt":"Extensions Jikkou allows implementing and configuring extensions, …","ref":"/jikkou/docs/concepts/extensions/","tags":["feature","extensions"],"title":"Extensions"},{"body":" SchemaRegistrySubject resources are used to define the subject schemas you want to manage on your SchemaRegistry. A SchemaRegistrySubject resource defines the schema, the compatibility level, and the references to be associated with a subject version.\nSchemaRegistrySubject Specification Here is the resource definition file for defining a SchemaRegistrySubject.\napiVersion: \"schemaregistry.jikkou.io/v1beta2\" # The api version (required) kind: \"SchemaRegistrySubject\" # The resource kind (required) metadata: name: \u003cThe name of the subject\u003e # (required) labels: { } annotations: { } spec: schemaRegistry: vendor: \u003cvendor_name\u003e # (optional) The vendor of the SchemaRegistry, e.g., Confluent, Karapace, etc compatibilityLevel: \u003ccompatibility_level\u003e # (optional) The schema compatibility level for this subject. schemaType: \u003cThe schema format\u003e # (required) Accepted values are: AVRO, PROTOBUF, JSON schema: $ref: \u003curl or path\u003e # references: # Specifies the names of referenced schemas (optional array). - name: \u003c\u003e # The name for the reference. subject: \u003c\u003e # The subject under which the referenced schema is registered. version: \u003c\u003e # The exact version of the schema under the registered subject. ] The metadata.name property is mandatory and specifies the name of the Subject.\nTo use the SchemaRegistry default values for the compatibilityLevel you can omit the property.\nExample Here is a simple example that shows how to define a single subject AVRO schema for type using the SchemaRegistrySubject resource type.\nfile: subject-user.yaml\n--- apiVersion: \"schemaregistry.jikkou.io/v1beta2\" kind: \"SchemaRegistrySubject\" metadata: name: \"User\" labels: { } annotations: schemaregistry.jikkou.io/normalize-schema: true spec: compatibilityLevel: \"FULL_TRANSITIVE\" schemaType: \"AVRO\" schema: $ref: ./user-schema.avsc file: user-schema.avsc\n--- { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"name\", \"type\": [ \"null\", \"string\" ], \"default\": null, }, { \"name\": \"favorite_number\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"favorite_color\", \"type\": [ \"null\", \"string\" ], \"default\": null } ] } Alternatively, we can directly pass the Avro schema as follows :\nfile: subject-user.yaml\n--- apiVersion: \"schemaregistry.jikkou.io/v1beta2\" kind: \"SchemaRegistrySubject\" metadata: name: \"User\" labels: { } annotations: schemaregistry.jikkou.io/normalize-schema: true spec: compatibilityLevel: \"FULL_TRANSITIVE\" schemaType: \"AVRO\" schema: | { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"name\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"favorite_number\", \"type\": [ \"null\", \"int\" ], \"default\": null }, { \"name\": \"favorite_color\", \"type\": [ \"null\", \"string\"], \"default\": null } ] } ","categories":"","description":"Learn how to manage SchemaRegistry Subjects.\n","excerpt":"Learn how to manage SchemaRegistry Subjects.\n","ref":"/jikkou/docs/user-guide/schema-registry/resources/subject/","tags":["feature","resources"],"title":"Schema Registry Subjects"},{"body":" KafkaTopic resources are used to define the topics you want to manage on your Kafka Cluster(s). A KafkaTopic resource defines the number of partitions, the replication factor, and the configuration properties to be associated to a topics.\nKafkaTopic Specification Here is the resource definition file for defining a KafkaTopic.\napiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaTopic\" # The resource kind (required) metadata: name: \u003cThe name of the topic\u003e # (required) labels: { } annotations: { } spec: partitions: \u003cNumber of partitions\u003e # (optional) replicas: \u003cNumber of replicas\u003e # (optional) configs: \u003cconfig_key\u003e: \u003cConfig Value\u003e # The topic config properties keyed by name to override (optional) configMapRefs: [ ] # The list of ConfigMap to be applied to this topic (optional) The metadata.name property is mandatory and specifies the name of the kafka topic.\nTo use the cluster default values for the number of partitions and replicas you can set the property spec.partitions and spec.replicas to -1.\nExample Here is a simple example that shows how to define a single YAML file containing two topic definition using the KafkaTopic resource type.\nfile: kafka-topics.yaml\n--- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopic metadata: name: 'my-topic-p1-r1' # Name of the topic labels: environment: example spec: partitions: 1 # Number of topic partitions (use -1 to use cluster default) replicas: 1 # Replication factor per partition (use -1 to use cluster default) configs: min.insync.replicas: 1 cleanup.policy: 'delete' --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopic metadata: name: 'my-topic-p2-r1' # Name of the topic labels: environment: example spec: partitions: 2 # Number of topic partitions (use -1 to use cluster default) replicas: 1 # Replication factor per partition (use -1 to use cluster default) configs: min.insync.replicas: 1 cleanup.policy: 'delete' See official Apache Kafka documentation for details about the topic-level configs.\nTips: Multiple topics can be included in the same YAML file by using --- lines. KafkaTopicList If you need to define multiple topics (e.g. using a template), it may be easier to use a KafkaTopicList resource.\nSpecification Here the resource definition file for defining a KafkaTopicList.\napiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaTopicList\" # The resource kind (required) metadata: # (optional) labels: { } annotations: { } items: [ ] # An array of KafkaTopic Example Here is a simple example that shows how to define a single YAML file containing two topic definitions using the KafkaTopicList resource type. In addition, the example uses a ConfigMap object to define the topic configuration only once.\nfile: kafka-topics.yaml\n--- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: KafkaTopicList metadata: labels: environment: example items: - metadata: name: 'my-topic-p1-r1' spec: partitions: 1 replicas: 1 configMapRefs: [ \"TopicConfig\" ] - metadata: name: 'my-topic-p2-r1' spec: partitions: 2 replicas: 1 configMapRefs: [ \"TopicConfig\" ] --- apiVersion: \"core.jikkou.io/v1beta2\" kind: ConfigMap metadata: name: 'TopicConfig' data: min.insync.replicas: 1 cleanup.policy: 'delete' ","categories":"","description":"Learn how to manage Kafka Topics.\n","excerpt":"Learn how to manage Kafka Topics.\n","ref":"/jikkou/docs/user-guide/kafka/resources/topics/","tags":["feature","resources"],"title":"Kafka Topics"},{"body":" About Jikkou What is Jikkou and why using it ? Contents Jikkou is an open-source tool to help you automate the management of the configurations that live on your Apache Kafka clusters. It was developed by Kafka ❤️ to make daily operations on an Apache Kafka cluster simpler for both developers and administrators.\nIt can efficiently manage configuration changes for Topics, ACLs, Quotas and more with the use of resource specification files. It is also applicable to quickly replicate the configuration of a production cluster to another with a few command lines or to initialize a new cluster for testing purpose.\nThe main usage scenarios Create new resource entities on an Apache Kafka cluster (i.e., Topics, ACLs, and Quotas). Update the configurations of existing resource entities. Delete resource entities which are not anymore managed. Describe all the configuration defined for Brokers. Describe all the configuration defined for Topics, ACLs, and Quotas. Core features that make it awesome Simple command line interface (CLI) for end user. Simple Java API on top of the Kafka’s Java AdminClient. Completely stateless and thus does not store any state (Basically: Your kafka cluster is the state of Jikkou). Pluggable validation rules to ensure that resources meet your requirement before being created or updated ona target cluster. Pluggable resource manager to extend Jikkou with cloud managed services for Apache Kafka which are supported out-of-the-box. Simple templating mechanism using Jinja notation. The story behind Jikkou Jikkou was initially created as a side project to help the development teams to quickly re-create topics on Apache Kafka clusters used for testing purpose. The goal was to ensure that environments was always cleanup and ready for running integration tests. Over time, new features have been added to Jikkou so that it can also be used by Kafka administrators on Kafka environments. Today we continue to make the tool evolve in open-source because we find that the solutions that have appeared over time in the Kafka ecosystem do not respond as well to Kafka developers and administrators. In addition, existing solutions are either designed to work only with Kubernetes or rely on dedicated services to manage the state of the solution.\nCredits Images used on this website documentation: Page Cover: Photo by Anton Ivanov on Unsplash. Contents View page source Edit this page Create child page Create documentation issue Create project issue Print entire section View page source Edit this page Create child page Create documentation issue Create project issue Print entire section ","categories":"","description":"What is Jikkou and why using it ?\n","excerpt":"What is Jikkou and why using it ?\n","ref":"/jikkou/about/","tags":"","title":"About Jikkou"},{"body":" The SchemaRegistryAclEntry resources are used to manage the Access Control Lists for Aiven Schema Registry service. A SchemaRegistryAclEntry resource defines the permission to be granted to a user for one or more Schema Registry Subjects.\nSchemaRegistryAclEntry Specification Here is the resource definition file for defining a SchemaRegistryAclEntry.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"SchemaRegistryAclEntry\" # The resource kind (required) metadata: labels: { } annotations: { } spec: permission: \u003c\u003e # The permission. Accepted values are: READ, WRITE, ADMIN username: \u003c\u003e # The username resource: \u003c\u003e # The Schema Registry ACL entry resource name pattern NOTE: The resource name pattern should be Config: or Subject:\u003csubject_name\u003e where subject_name must consist of alpha-numeric characters, underscores, dashes, dots and glob characters * and ?.\nExample Here is an example that shows how to define a simple ACL entry using the SchemaRegistryAclEntry resource type.\nfile: schema-registry-acl-entry.yaml\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"SchemaRegistryAclEntry\" metadata: labels: { } annotations: { } spec: permission: \"WRITE\" username: \"alice\" topic: \"Subject:*\" SchemaRegistryAclEntryList If you need to define multiple ACL entries (e.g. using a template), it may be easier to use a SchemaRegistryAclEntryList resource.\nSpecification Here the resource definition file for defining a SchemaRegistryAclEntryList.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" # The api version (required) kind: \"SchemaRegistryAclEntryList\" # The resource kind (required) metadata: # (optional) labels: { } annotations: { } items: [ ] # An array of SchemaRegistryAclEntry Example Here is a simple example that shows how to define a single YAML file containing two ACL entry definitions using the SchemaRegistryAclEntryList resource type.\n--- apiVersion: \"kafka.aiven.io/v1beta1\" kind: \"SchemaRegistryAclEntryList\" items: - spec: permission: \"READ\" username: \"alice\" resource: \"Config:*\" - spec: permission: \"WRITE\" username: \"alice\" topic: \"Subject:*\" ","categories":"","description":"Learn how to manage Access Control Lists for Aiven Schema Registry\n","excerpt":"Learn how to manage Access Control Lists for Aiven Schema Registry\n","ref":"/jikkou/docs/user-guide/aiven/resources/kafka-schema-registry-acl/","tags":["feature","resources"],"title":"ACL for Aiven Schema Registry"},{"body":" KafkaPrincipalAuthorization resources are used to define Access Control Lists (ACLs) for principals authenticated to your Kafka Cluster.\nJikkou can be used to describe all ACL policies that need to be created on Kafka Cluster\nKafkaPrincipalAuthorization Specification --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalAuthorization\" metadata: name: \"User:Alice\" spec: roles: [ ] # List of roles to be added to the principal (optional) acls: # List of KafkaPrincipalACL (required) - resource: type: \u003cThe type of the resource\u003e # (required) pattern: \u003cThe pattern to be used for matching resources\u003e # (required) patternType: \u003cThe pattern type\u003e # (required) type: \u003cThe type of this ACL\u003e # ALLOW or DENY (required) operations: [ ] # Operation that will be allowed or denied (required) host: \u003cHOST\u003e # IP address from which principal will have access or will be denied (optional) For more information on how to define authorization and ACLs, see the official Apache Kafka documentation: Security\nOperations The list below describes the valid values for the spec.acls.[].operations property :\nREAD WRITE CERATE DELETE ALTER DESCRIBE CLUSTER_ACTION DESCRIBE_CONFIGS ALTER_CONFIGS IDEMPOTENT_WRITE CREATE_TOKEN DESCRIBE_TOKENS ALL For more information see official Apache Kafka documentation: Operations in Kafka\nResource Types The list below describes the valid values for the spec.acls.[].resource.type property :\nTOPIC GROUP CLUSTER USER TRANSACTIONAL_ID For more information see official Apache Kafka documentation: Resources in Kafka\nPattern Types The list below describes the valid values for the spec.acls.[].resource.patternType property :\nLITERAL: Use to allow or denied a principal to have access to a specific resource name. MATCH: Use to allow or denied a principal to have access to all resources matching the given regex. PREFIXED: Use to allow or denied a principal to have access to all resources having the given prefix. Example --- apiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaPrincipalAuthorization\" # The resource kind (required) metadata: name: \"User:Alice\" spec: acls: - resource: type: 'topic' pattern: 'my-topic-' patternType: 'PREFIXED' type: \"ALLOW\" operations: [ 'READ', 'WRITE' ] host: \"*\" - resource: type: 'topic' pattern: 'my-other-topic-.*' patternType: 'MATCH' type: 'ALLOW' operations: [ 'READ' ] host: \"*\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalAuthorization\" metadata: name: \"User:Bob\" spec: acls: - resource: type: 'topic' pattern: 'my-topic-' patternType: 'PREFIXED' type: 'ALLOW' operations: [ 'READ', 'WRITE' ] host: \"*\" KafkaPrincipalRole Specification apiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaPrincipalRole\" # The resource kind (required) metadata: name: \u003cName of role\u003e # The name of the role (required) spec: acls: [ ] # A list of KafkaPrincipalACL (required) Example --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalRole\" metadata: name: \"KafkaTopicPublicRead\" spec: acls: - type: \"ALLOW\" operations: [ 'READ' ] resource: type: 'topic' pattern: '/public-([.-])*/' patternType: 'MATCH' host: \"*\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalRole\" metadata: name: \"KafkaTopicPublicWrite\" spec: acls: - type: \"ALLOW\" operations: [ 'WRITE' ] resource: type: 'topic' pattern: '/public-([.-])*/' patternType: 'MATCH' host: \"*\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalAuthorization\" metadata: name: \"User:Alice\" spec: roles: - \"KafkaTopicPublicRead\" - \"KafkaTopicPublicWrite\" --- apiVersion: \"kafka.jikkou.io/v1beta2\" kind: \"KafkaPrincipalAuthorization\" metadata: name: \"User:Bob\" spec: roles: - \"KafkaTopicPublicRead\" ","categories":"","description":"Learn how to manage Kafka Authorizations and ACLs. \n","excerpt":"Learn how to manage Kafka Authorizations and ACLs. \n","ref":"/jikkou/docs/user-guide/kafka/resources/acls/","tags":["feature","resources"],"title":"Kafka Authorizations"},{"body":"Jikkou is an open source project, and we love getting patches and contributions to make Jikkou and its docs even better.\nContributing to Jikkou The Jikkou project itself lives in https://github.com/streamthoughts/jikkou\nCode reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.\nCreating issues Alternatively, if there’s something you’d like to see in Jikkou (or if you’ve found something that isn’t working the way you’d expect), but you’re not sure how to fix it yourself, please create an issue.\n","categories":"","description":"How to contribute to Jikkou\n","excerpt":"How to contribute to Jikkou\n","ref":"/jikkou/docs/community/contribution-guidelines/","tags":"","title":"Contribution Guidelines"},{"body":"Welcome to Jikkou Welcome to the Jikkou documentation. Jikkou is the tool to automate the management of the configurations that live on your Apache Kafka clusters. This guide shows you how to get started managing configurations for Topics, ACLS, Quotas, and more in the simplest and most seamless way possible.\nIs Jikkou for me ? Jikkou is particularly useful for small development teams that want to quickly automate the creation and updating of their topics without having to implement complex solutions. But, it can be also very effective in larger contexts, where the configuration of your topics for all your projects are managed by a single centralized administration team.\nIn addition, it is built on top of the Kafka’s Java AdminClient. Thus, it works out-of-the-box with most the Apache Kafka distributions and cloud provider managed services (e.g., Aiven, Confluent Cloud, etc). However, you may find some limitations with some managed services for Apache Kafka depending on which APIs are allowed to be used. Indeed, some providers limit for example some config properties to be overloaded or use own ACL management mechanisms.\n","categories":"","description":"","excerpt":"Welcome to Jikkou Welcome to the Jikkou documentation. Jikkou is the …","ref":"/jikkou/docs/","tags":"","title":"Documentation"},{"body":" KafkaClientQuota resources are used to define the quota limits to be applied on Kafka consumers and producers. A KafkaClientQuota resource can be used to apply limit to consumers and/or producers identified by a client-id or a user principal.\nKafkaClientQuota Specification Here is the resource definition file for defining a KafkaClientQuota.\napiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaClientQuota\" # The resource kind (required) metadata: # (optional) labels: { } annotations: { } spec: type: \u003cThe quota type\u003e # (required) entity: clientId: \u003cThe id of the client\u003e # (required depending on the quota type). user: \u003cThe principal of the user\u003e # (required depending on the quota type). configs: requestPercentage: \u003cThe quota in percentage (%) of total requests\u003e # (optional) producerByteRate: \u003cThe quota in bytes for restricting data production\u003e # (optional) consumerByteRate: \u003cThe quota in bytes for restricting data consumption\u003e # (optional) Quota Types The list below describes the supported quota types:\nUSERS_DEFAULT: Set default quotas for all users. USER: Set quotas for a specific user principal. USER_CLIENT: Set quotas for a specific user principal and a specific client-id. USER_ALL_CLIENTS: Set default quotas for a specific user and all clients. CLIENT: Set default quotas for a specific client. CLIENTS_DEFAULT: Set default quotas for all clients. Example Here is a simple example that shows how to define a single YAML file containing two quota definitions using the KafkaClientQuota resource type.\nfile: kafka-quotas.yaml\n--- apiVersion: 'kafka.jikkou.io/v1beta2' kind: 'KafkaClientQuota' metadata: labels: { } annotations: { } spec: type: 'CLIENT' entity: clientId: 'my-client' configs: requestPercentage: 10 producerByteRate: 1024 consumerByteRate: 1024 --- apiVersion: 'kafka.jikkou.io/v1beta2' kind: 'KafkaClientQuota' metadata: labels: { } annotations: { } spec: type: 'USER' entity: user: 'my-user' configs: requestPercentage: 10 producerByteRate: 1024 consumerByteRate: 1024 KafkaClientQuotaList If you need to define multiple topics (e.g. using a template), it may be easier to use a KafkaClientQuotaList resource.\nSpecification Here the resource definition file for defining a KafkaTopicList.\napiVersion: \"kafka.jikkou.io/v1beta2\" # The api version (required) kind: \"KafkaClientQuotaList\" # The resource kind (required) metadata: # (optional) name: \u003cThe name of the topic\u003e labels: { } annotations: { } items: [ ] # An array of KafkaClientQuota Example Here is a simple example that shows how to define a single YAML file containing two KafkaClientQuota definition using the KafkaClientQuotaList resource type.\napiVersion: 'kafka.jikkou.io/v1beta2' kind: 'KafkaClientQuotaList' metadata: labels: { } annotations: { } items: - spec: type: 'CLIENT' entity: clientId: 'my-client' configs: requestPercentage: 10 producerByteRate: 1024 consumerByteRate: 1024 - spec: type: 'USER' entity: user: 'my-user' configs: requestPercentage: 10 producerByteRate: 1024 consumerByteRate: 1024 ","categories":"","description":"Learn how to manage Kafka Client Quotas\n","excerpt":"Learn how to manage Kafka Client Quotas\n","ref":"/jikkou/docs/user-guide/kafka/resources/quotas/","tags":["feature","resources"],"title":"Kafka Quotas"},{"body":"Jikkou ships with the following built-in validations:\nNo validation\n","categories":"","description":"Learn how to use the built-in validations provided by the extensions for Aiven.\n","excerpt":"Learn how to use the built-in validations provided by the extensions …","ref":"/jikkou/docs/user-guide/aiven/validations/","tags":["feature","extensions"],"title":"Validations"},{"body":"Jikkou ships with the following built-in validations:\nTopics NoDuplicateTopicsAllowedValidation (auto registered)\nTopicConfigKeysValidation (auto registered)\ntype = io.streamthoughts.jikkou.kafka.validation.TopicConfigKeysValidation The TopicConfigKeysValidation allows checking if the specified topic configs are all valid.\nTopicMinNumPartitions type = io.streamthoughts.jikkou.kafka.validation.TopicMinNumPartitionsValidation The TopicMinNumPartitions allows checking if the specified number of partitions for a topic is not less than the minimum required.\nConfiguration\nName Type Description Default topicMinNumPartitions Int Minimum number of partitions allowed TopicMaxNumPartitions type = io.streamthoughts.jikkou.kafka.validation.TopicMaxNumPartitions The TopicMaxNumPartitions allows checking if the number of partitions for a topic is not greater than the maximum configured.\nConfiguration\nName Type Description Default topicMaxNumPartitions Int Maximum number of partitions allowed TopicMinReplicationFactor type = io.streamthoughts.jikkou.kafka.validation.TopicMinReplicationFactor The TopicMinReplicationFactor allows checking if the specified replication factor for a topic is not less than the minimum required.\nConfiguration\nName Type Description Default topicMinReplicationFactor Int Minimum replication factor allowed TopicMaxReplicationFactor type = io.streamthoughts.jikkou.kafka.validation.TopicMaxReplicationFactor The TopicMaxReplicationFactor allows checking if the specified replication factor for a topic is not greater than the maximum configured.\nConfiguration\nName Type Description Default topicMaxReplicationFactor Int Maximum replication factor allowed TopicNamePrefix type = io.streamthoughts.jikkou.kafka.validation.TopicNamePrefix The TopicNamePrefix allows checking if the specified name for a topic starts with one of the configured suffixes.\nConfiguration\nName Type Description Default topicNamePrefixes List List of topic name prefixes allows TopicNameSuffix type = io.streamthoughts.jikkou.kafka.validation.TopicNameSuffix The TopicNameSuffix allows checking if the specified name for a topic ends with one of the configured suffixes.\nConfiguration\nName Type Description Default topicNameSuffixes List List of topic name suffixes allows ACLs NoDuplicateUsersAllowedValidation (auto registered)\nNoDuplicateRolesAllowedValidation (auto registered)\nQuotas QuotasEntityValidation (auto registered)\n","categories":"","description":"Learn how to use the built-in validations provided by the extensions for Apache Kafka.\n","excerpt":"Learn how to use the built-in validations provided by the extensions …","ref":"/jikkou/docs/user-guide/kafka/validations/","tags":["feature","extensions"],"title":"Validations"},{"body":"Jikkou ships with the following built-in validations:\nSubject SchemaCompatibilityValidation type = io.streamthoughts.jikkou.schema.registry.validation.SchemaCompatibilityValidation The SchemaCompatibilityValidation allows testing the compatibility of the schema with the latest version already registered in the Schema Registry using the provided compatibility-level.\nAvroSchemaValidation The AvroSchemaValidation allows checking if the specified Avro schema matches some specific avro schema definition rules;\ntype = io.streamthoughts.jikkou.schema.registry.validation.AvroSchemaValidation Configuration\nName Type Description Default fieldsMustHaveDoc Boolean Verify that all record fields have a doc property false fieldsMustBeNullable Boolean Verify that all record fields are nullable false fieldsMustBeOptional Boolean Verify that all record fields are optional false ","categories":"","description":"Learn how to use the built-in validations provided by the extensions for Schema Registry.\n","excerpt":"Learn how to use the built-in validations provided by the extensions …","ref":"/jikkou/docs/user-guide/schema-registry/validations/","tags":["feature","extensions"],"title":"Validations"},{"body":" Here, you will find information about the annotations provided by the Aiven extension for Jikkou.\nList of built-in annotations kafka.aiven.io/acl-entry-id Used by jikkou.\nThe annotation is automatically added by Jikkou to describe the ID of an ACL entry.\n","categories":"","description":"Learn how to use the metadata annotations provided by the extensions for Aiven.\n","excerpt":"Learn how to use the metadata annotations provided by the extensions …","ref":"/jikkou/docs/user-guide/aiven/annotations/","tags":"","title":"Annotations"},{"body":" Here, you will find information about the annotations provided the Apache Kafka extension for Jikkou.\nList of built-in annotations kafka.jikkou.io/cluster-id Used by jikkou.\nThe annotation is automatically added by Jikkou to a describe object part of an Apache Kafka cluster.\n","categories":"","description":"Learn how to use the metadata annotations provided by the extensions for Apache Kafka.\n","excerpt":"Learn how to use the metadata annotations provided by the extensions …","ref":"/jikkou/docs/user-guide/kafka/annotations/","tags":"","title":"Annotations"},{"body":" Here, you will find information about the annotations provided by the Schema Registry extension for Jikkou.\nList of built-in annotations schemaregistry.jikkou.io/url Used by jikkou.\nThe annotation is automatically added by Jikkou to describe the SchemaRegistry URL from which a subject schema is retrieved.\nschemaregistry.jikkou.io/schema-version Used by jikkou.\nThe annotation is automatically added by Jikkou to describe the version of a subject schema.\nschemaregistry.jikkou.io/schema-id Used by jikkou.\nThe annotation is automatically added by Jikkou to describe the version of a subject id.\nschemaregistry.jikkou.io/normalize-schema Used on: schemaregistry.jikkou.io/v1beta2:SchemaRegistrySubject\nThis annotation can be used to normalize the schema on SchemaRegistry server side. Note, that Jikkou will attempt to normalize AVRO and JSON schema.\nSee: Confluent SchemaRegistry API Reference\nschemaregistry.jikkou.io/permanante-delete Used on: schemaregistry.jikkou.io/v1beta2:SchemaRegistrySubject\nThe annotation can be used to specify a hard delete of the subject, which removes all associated metadata including the schema ID. The default is false. If the flag is not included, a soft delete is performed. You must perform a soft delete first, then the hard delete.\nSee: Confluent SchemaRegistry API Reference\n","categories":"","description":"Learn how to use the metadata annotations provided by the extensions for Schema Registry.\n","excerpt":"Learn how to use the metadata annotations provided by the extensions …","ref":"/jikkou/docs/user-guide/schema-registry/annotations/","tags":"","title":"Annotations"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/tags/concept/","tags":"","title":"concept"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/tags/docs/","tags":"","title":"docs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/tags/extension/","tags":"","title":"extension"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/tags/extensions/","tags":"","title":"extensions"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/tags/feature/","tags":"","title":"feature"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/tags/how-to/","tags":"","title":"how-to"},{"body":" Jikkou (jikkō / 実行)! Streamline the management of the configurations that live on your data streams platform. Learn More Download Why Jikkou? Jikkou is an open-source tool to help you automate the management of the configurations that live on your Apache Kafka clusters. It was developed by Kafka ❤️ to make daily operations on an Apache Kafka cluster simpler for both developers and administrators.\nIt can efficiently manage configuration changes for Topics, ACLs, Quotas and more with the use of resource definition files. It is also applicable to quickly replicate the configuration of a production cluster to another with a few command lines or to initialize a new cluster for testing purpose.\nResource definition are standard YAML files which are easy to create, version, share, and publish. For this reason, Jikkou is usually used part of CI/CD pipelines allowing you to version and ship your Apache Kafka resources directly from Git (more on GitOps).\nJikkou is distributed under the Apache 2.0 license. Apache, Apache Kafka, Kafka, and associated open source project names are trademarks of the Apache Software Foundation.\nContributions welcome Want to join the fun on Github? New users are always welcome!\nContribute Support Jikkou Team Add a star to the GitHub project, it only takes 5 seconds!\nStar ","categories":"","description":"","excerpt":" Jikkou (jikkō / 実行)! Streamline the management of the configurations …","ref":"/jikkou/","tags":"","title":"Jikkou"},{"body":" This transformation can be used to enforce a maximum value for the number of partitions of kafka topics.\nConfiguration Name Type Description Default maxNumPartitions Int maximum value for the number of partitions to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMaxNumPartitions priority = 100 config = { maxNumPartitions = 50 } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a maximum value for the …","ref":"/jikkou/docs/user-guide/kafka/transformations/kafkatopicmaxnumpartitions/","tags":"","title":"KafkaTopicMaxNumPartitions"},{"body":" This transformation can be used to enforce a maximum value for the retention.ms property of kafka topics.\nConfiguration Name Type Description Default maxRetentionMs Int Minimum value of retention.ms to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMinRetentionMsTransformation priority = 100 config = { maxRetentionMs = 2592000000 # 30 days } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a maximum value for the …","ref":"/jikkou/docs/user-guide/kafka/transformations/kafkatopicmaxretentionms/","tags":"","title":"KafkaTopicMaxRetentionMs"},{"body":" This transformation can be used to enforce a minimum value for the min.insync.replicas property of kafka topics.\nConfiguration Name Type Description Default minInSyncReplicas Int Minimum value of min.insync.replicas to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMinInSyncReplicasTransformation priority = 100 config = { minInSyncReplicas = 2 } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a minimum value for the …","ref":"/jikkou/docs/user-guide/kafka/transformations/kafkatopicmininsyncreplicas/","tags":"","title":"KafkaTopicMinInSyncReplicas"},{"body":" This transformation can be used to enforce a minimum value for the replication factor of kafka topics.\nConfiguration Name Type Description Default minReplicationFactor Int Minimum value of replication factor to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMinReplicasTransformation priority = 100 config = { minReplicationFactor = 3 } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a minimum value for the …","ref":"/jikkou/docs/user-guide/kafka/transformations/kafkatopicminreplicas/","tags":"","title":"KafkaTopicMinReplicas"},{"body":" This transformation can be used to enforce a minimum value for the retention.ms property of kafka topics.\nConfiguration Name Type Description Default minRetentionMs Int Minimum value of retention.ms to be used for Kafka Topics Example jikkou { transformations: [ { type = io.streamthoughts.jikkou.kafka.transform.KafkaTopicMinRetentionMsTransformation priority = 100 config = { minRetentionMs = 604800000 # 7 days } } ] } ","categories":"","description":"","excerpt":" This transformation can be used to enforce a minimum value for the …","ref":"/jikkou/docs/user-guide/kafka/transformations/kafkatopicminretentionms/","tags":"","title":"KafkaTopicMinRetentionMs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/tags/resources/","tags":"","title":"resources"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/jikkou/tags/","tags":"","title":"Tags"}]